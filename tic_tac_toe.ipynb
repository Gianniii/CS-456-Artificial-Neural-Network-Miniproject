{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "\n",
    "coords = lambda pos: (int(pos / 3), pos % 3) \n",
    "\n",
    "def eps_greedy(Q, state, grid, epsilon):\n",
    "    possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "    if np.random.random() < epsilon:\n",
    "        position = np.random.choice(possible_moves)\n",
    "    else:\n",
    "        possible = [int(elem) for elem in np.argsort(Q[state]) if elem in possible_moves]\n",
    "        position = possible[-1] if possible is not [] else np.random.choice(possible_moves) \n",
    "    return (int(position / 3), position % 3) \n",
    "\n",
    "#Q learning with greedy\n",
    "#Dont necessariliy need to train with player_epsilon = 0.5\n",
    "def q_learning(env, num_episodes, discount_factor=0.99, alpha=0.05, epsilon=0.3, player_epsilon=0.5):\n",
    "    Q = defaultdict(lambda: np.zeros(9))\n",
    "    episode_lengths = np.zeros(num_episodes)\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    Turns = np.array(['X','O'])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            episode_lengths[i_episode] += 1\n",
    "            if env.current_player == Turns[0]:\n",
    "                #move = eps_greedy(Q, str(grid), grid, epsilon=epsilon - epsilon/num_episodes*i_episode)\n",
    "                \n",
    "                #Remark: for question 2.1 epsilon needs to be constant and not decreasing\n",
    "                move = eps_greedy(Q, str(grid), grid, epsilon = epsilon)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            \n",
    "            new_grid, end, winner = env.step(move, print_grid=False)\n",
    "            episode_rewards[i_episode] += env.reward(env.current_player)\n",
    "#             if end: \n",
    "#                 Q[str(grid)][move[0] * 3 + move[1]] = env.reward(Turns[0])\n",
    "#             else: \n",
    "#             td_target = (discount_factor * np.max(Q[str(new_grid)])) * alpha \n",
    "#             td = (1 - alpha) * Q[str(grid)][move[0] * 3 + move[1]]\n",
    "#             Q[str(grid)][move[0] * 3 + move[1]] = td + td_target \n",
    "            if env.current_player == Turns[0]:\n",
    "                value = Q[str(grid)][move[0] * 3 + move[1]]\n",
    "                Q[str(grid)][move[0] * 3 + move[1]] += alpha * (env.reward(Turns[0]) + discount_factor * np.max(Q[str(new_grid)]) - value)\n",
    "            grid = new_grid \n",
    "    \n",
    "    return Q, episode_lengths, episode_rewards\n",
    "\n",
    "Q, episode_lengths, episode_rewards = q_learning(env, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uodate Q values at every move (optimal player and q-learning player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def q_eval(Q, other_player_epsilon=0.5, games=20000):\n",
    "    env.reset()\n",
    "    total_reward = []\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win = 0 \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "                possible = [int(elem) for elem in np.argsort(Q[str(grid)]) if elem in possible_moves]\n",
    "                position = possible[-1] if possible is not [] else np.random.choice(possible_moves)\n",
    "                move = (int(position / 3), position % 3) \n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            if winner == Turns[0]: \n",
    "                count_win += 1 \n",
    "            if end:\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ' +  Turns[0])\n",
    "#                 print('Optimal player = ' +  Turns[1])\n",
    "#                 env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "        \n",
    "    return np.mean(total_reward), count_win / games \n",
    "\n",
    "\n",
    "reward, score = q_eval(Q, other_player_epsilon = 1., games = 2000)\n",
    "print(score, reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = TictactoeEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#Modifies state to specification given in project description.\n",
    "#Takes a grid and returns a 3x3x2 tensor which contains only values 0 or 1\n",
    "# such that [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent\n",
    "def modify_state_to_description(grid, my_turn_sign): \n",
    "    if('X' == my_turn_sign): #if i am first player to play\n",
    "        my_array =  np.where(grid == -1, 0, grid) #replace -1's with 0(remove other players moves)\n",
    "        other_array = np.where(grid == 1, 0, grid)  #replace 1's with 0\n",
    "        other_array = np.where(other_array == -1, 1, other_array) \n",
    "    else: #-1 = my_moves\n",
    "        my_array = np.where(grid == 1, 0, grid) #remove other players moves\n",
    "        my_array = np.where(my_array == -1, 1, my_array) #change -1 to 1's \n",
    "        other_array = np.where(grid == -1, 0, grid)\n",
    "    #print(\"grid\")#print(grid)\n",
    "    state = np.stack((my_array, other_array))\n",
    "    #print(\"state\")print(state)\n",
    "    return state\n",
    "\n",
    "#Given the state, prepares a tensor for the neural network\n",
    "def get_tensor_for_neural_net(state):\n",
    "    state = torch.from_numpy(state)\n",
    "    state = torch.flatten(state)\n",
    "    return state.unsqueeze(0).type(torch.float32) \n",
    "\n",
    "#Given integer representatin of a move, return tuple representation of move\n",
    "def get_move_from_position(position):\n",
    "    move = (int(position / 3), int(position) % 3)\n",
    "    return move\n",
    "\n",
    "def move_is_legal(move, grid):\n",
    "    possible_moves = np.where(np.ravel(grid.ravel()) == 0)[0]   \n",
    "    return move[0]*3 + move[1] in possible_moves\n",
    "\n",
    "#first player alternates each game\n",
    "def assign_players(turns, i_episode):\n",
    "    if(i_episode > 0):\n",
    "        turns = np.roll(turns, 1) \n",
    "    return turns[0], turns[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net from description\n",
    "# (fully connected, 2 hidden layers w 128 neurons each and relu, 3x3x2 inputs and 9 outputs)\n",
    "# (output layer uses linear activation function)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs=3*3*2, n_outputs=9):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_outputs)\n",
    "\n",
    "    # From description states are 3 ×3 × 2 tensor where\n",
    "    # where [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(state)\n",
    "            #print(output)\n",
    "            return output.max(1)[1].view(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNActor(): \n",
    "    \n",
    "    def __init__(self, device, batch_size, gamma, memory_size):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "\n",
    "        self.policy = Policy().to(self.device)\n",
    "        self.target = Policy().to(self.device)\n",
    "        self.target.load_state_dict(self.policy.state_dict()) #copy state from policy to target\n",
    "        self.target.eval() #put into eval state(BN, Dropout ect.. are now turned off)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=5e-4) #Description: Adam optimizer with lr = 5e-4\n",
    "        \n",
    "\n",
    "    # Taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=self.device,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    #Select action according to epsilon greedy policy.\n",
    "    def act(self, state: torch.tensor, eps_min, eps_max, n, num_exploratory_games): \n",
    "        sample = random.random()\n",
    "        \n",
    "        #Epsilon update function given in description\n",
    "        eps_threshold = max(eps_min, eps_max*(1 - n/num_exploratory_games))\n",
    "        #eps_threshold = FIXED_EPS_THRESHOLD #Some questions ask for fixed threshold\n",
    "        \n",
    "        # greedy \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return get_move_from_position(self.policy.act(state)), 0\n",
    "        else:\n",
    "            return get_move_from_position(torch.tensor([[random.randrange(9)]],dtype=torch.long)), 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For batch of 5000 games:  Wins: 1226, Losses: 304, Ties: 146 Invalid moves: 3324, Percentage of random moves: 0.6715426471627447\n",
      "For batch of 5000 games:  Wins: 2204, Losses: 219, Ties: 193 Invalid moves: 2384, Percentage of random moves: 0.40355793040531734\n",
      "For batch of 5000 games:  Wins: 3480, Losses: 180, Ties: 259 Invalid moves: 1081, Percentage of random moves: 0.15367082533589252\n",
      "For batch of 5000 games:  Wins: 3791, Losses: 171, Ties: 258 Invalid moves: 780, Percentage of random moves: 0.09647779479326186\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "#Training setup\n",
    "NUM_GAMES = 20000   # Description asks to train on 20k games\n",
    "BATCH_SIZE = 64     # Description sets batch size to 64\n",
    "GAMMA = 0.99        # Siscout factor Description sets to 0.99\n",
    "EPS_MAX = 0.8       # Description suggests this value\n",
    "EPS_MIN = 0.1       # Description suggests this value\n",
    "NUM_EXPLORATORY_GAMES = 15000 # Play with this value between [1, 40000], has big impact on how fast we learn\n",
    "FIXED_EPS_THRESHOLD = 0.3 \n",
    "TARGET_UPDATE = 500 # Description: update target_net every 500 games\n",
    "MEMORY_SIZE = 10000 # Description sets memory size to 10000\n",
    "other_player_epsilon = 1.0\n",
    "\n",
    "wins, ties, losses, invalid_moves, random_moves, total_moves_player_1 = 0, 0, 0, 0, 0, 0\n",
    "was_invalid = False\n",
    "Turns = np.array(['X','O'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_actions = 9\n",
    "\n",
    "#TODO do this in DQNActor class so can use DQNActor just like i ust OptimalPlayer, with extra actor.optimize() call\n",
    "#policy = Policy().to(device)\n",
    "#target = Policy().to(device)\n",
    "#target.load_state_dict(policy.state_dict()) #copy state from policy to target\n",
    "#target.eval() #put into eval state(BN, Dropout ect.. are now turned off)\n",
    "\n",
    "#optimizer = optim.Adam(policy.parameters(), lr=5e-4) #Description: Adam optimizer with lr = 5e-4\n",
    "#memory = ReplayMemory(10000) #Description: gives buffer of 10'000\n",
    "\n",
    "actor1 = DQNActor(device = device, batch_size = BATCH_SIZE, gamma = GAMMA, memory_size = MEMORY_SIZE);\n",
    "\n",
    "env = TictactoeEnv()\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "# Main Training loop\n",
    "# Train over a number of games\n",
    "for i_episode in range(NUM_GAMES):\n",
    "   \n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    grid, end, winner = env.observe()\n",
    "    player1, player2 = assign_players(Turns, i_episode) #first player changes every game\n",
    "    \n",
    "    other_player = OptimalPlayer(epsilon=other_player_epsilon, player=player2)\n",
    "    \n",
    "    #Loop until game ends\n",
    "    for t in count():\n",
    "        #print(\"grid\")#print(grid)#print(\"state\")#print(state)\n",
    "        state = get_tensor_for_neural_net(modify_state_to_description(np.array(grid, dtype=np.float32), env.current_player)) #print(state)\n",
    "        \n",
    "        # Select and perform an action\n",
    "        if env.current_player == player1:\n",
    "            move, rand = actor1.act(state, EPS_MIN, EPS_MAX, i_episode, NUM_EXPLORATORY_GAMES) \n",
    "            random_moves += rand\n",
    "            total_moves_player_1  += 1\n",
    "        else:  \n",
    "            move = other_player.act(grid)\n",
    "\n",
    "        #From description: if move puts us in an illegal state, we end and give reward = -1.\n",
    "        #(instead of forcing player to make legal moves)\n",
    "        if move_is_legal(move, grid) == False:\n",
    "            #print(\"not possible move\")\n",
    "            reward = -1\n",
    "            end = True\n",
    "            invalid_moves+=1\n",
    "            was_invalid = True\n",
    "        else:\n",
    "            #play move and get reward (Does it make sense to store also other players moves? )\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            reward = env.reward(Turns[0]) \n",
    "\n",
    "        # Observe next state\n",
    "        if not end:\n",
    "            next_state = modify_state_to_description(np.array(grid, dtype=np.float32), env.current_player)  \n",
    "            next_state = get_tensor_for_neural_net(next_state)\n",
    "        #game has ended\n",
    "        else:\n",
    "            #count wins losses ties \n",
    "            if was_invalid == False:\n",
    "                if winner == Turns[0]: \n",
    "                    wins += 1 \n",
    "                elif str(winner) == \"None\":\n",
    "                    ties += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "            was_invalid = False #reset boolean\n",
    "            next_state = None #because finish game\n",
    "       \n",
    "        # Store the transition in memory\n",
    "        action = torch.tensor([[move[0] * 3 + move[1]]]) #move\n",
    "        actor1.memory.push(state, action, next_state, torch.tensor([reward]))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network) of given actors\n",
    "        actor1.optimize_model()\n",
    "        \n",
    "        if end:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "    # Update the target network(s), copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        actor1.target.load_state_dict(actor1.policy.state_dict())\n",
    "    \n",
    "    # Evaluate training\n",
    "    if (i_episode+1) % 5000 == 0 and i_episode != 1:\n",
    "        print(\"For batch of 5000 games: \" + \" Wins: \" + str(wins) + \", Losses: \" + str(losses) +\n",
    "              \", Ties: \" + str(ties) + \" Invalid moves: \" + str(invalid_moves)\n",
    "              + \", Percentage of random moves: \" + str(random_moves/total_moves_player_1))\n",
    "        wins, ties, losses, invalid_moves, random_moves, total_moves_player_1 = 0, 0, 0, 0, 0, 0 \n",
    "\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO How to evaluate invalid moves ? \n",
    "def dqn_eval(other_player_epsilon=0.5, games=20000):\n",
    "    total_reward = []\n",
    "    env.reset()\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win, none_win, loose = 0, 0, 0 \n",
    "    winner_list = [] \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        \n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                state = modify_state_to_description(np.array([[grid]], dtype=np.int32), Turns[0])\n",
    "                state = torch.from_numpy(state)\n",
    "                # Resize, and add a batch dimension (BCHW)\n",
    "                state = state.unsqueeze(0).type(torch.float32) #mmh suspicious\n",
    "                position = select_model_action(policy, state)\n",
    "                move = (int(position / 3), int(position) % 3)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            #at this point should not be making invalid moves... because player has trained\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            \n",
    "            if end:\n",
    "                if winner == Turns[0]: \n",
    "                    count_win += 1 \n",
    "                elif str(winner) == \"None\":\n",
    "                    none_win += 1\n",
    "                else:\n",
    "                    loose += 1\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ', Turns[0])\n",
    "#                 print(f'Another player with epsilon {other_player_epsilon} = ', Turns[1])\n",
    "        #             env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "    return count_win / games, none_win / games, loose / games, np.mean(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "There is already a chess on position (0, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 60'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000056?line=0'>1</a>\u001b[0m score \u001b[39m=\u001b[39m dqn_eval(\u001b[39m1.\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000056?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWins of DQN: \u001b[39m\u001b[39m{\u001b[39;00mscore[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, wins None: \u001b[39m\u001b[39m{\u001b[39;00mscore[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, wins Another player: \u001b[39m\u001b[39m{\u001b[39;00mscore[\u001b[39m2\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 59'\u001b[0m in \u001b[0;36mdqn_eval\u001b[0;34m(other_player_epsilon, games)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=19'>20</a>\u001b[0m     move \u001b[39m=\u001b[39m player\u001b[39m.\u001b[39mact(grid)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=20'>21</a>\u001b[0m \u001b[39m#at this point should not be making invalid moves... because player has trained\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=21'>22</a>\u001b[0m grid, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(move, print_grid\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=22'>23</a>\u001b[0m total_reward\u001b[39m.\u001b[39mappend(env\u001b[39m.\u001b[39mreward(env\u001b[39m.\u001b[39mcurrent_player))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:61\u001b[0m, in \u001b[0;36mTictactoeEnv.step\u001b[0;34m(self, position, print_grid)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=58'>59</a>\u001b[0m     position \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(position)\n\u001b[1;32m     <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid[position] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=60'>61</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThere is already a chess on position \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(position))\n\u001b[1;32m     <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=62'>63</a>\u001b[0m \u001b[39m# place a chess on the position\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=63'>64</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid[position] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer2value[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_player]\n",
      "\u001b[0;31mValueError\u001b[0m: There is already a chess on position (0, 1)."
     ]
    }
   ],
   "source": [
    "score = dqn_eval(0.5, 1000)\n",
    "print(f\"Wins of DQN: {score[0]}, wins None: {score[1]}, wins Another player: {score[2]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
