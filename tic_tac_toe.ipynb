{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "\n",
    "coords = lambda pos: (int(pos / 3), pos % 3) \n",
    "\n",
    "def eps_greedy(Q, state, grid, epsilon):\n",
    "    possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "    if np.random.random() < epsilon:\n",
    "        position = np.random.choice(possible_moves)\n",
    "    else:\n",
    "        possible = [int(elem) for elem in np.argsort(Q[state]) if elem in possible_moves]\n",
    "        position = possible[-1] if possible is not [] else np.random.choice(possible_moves) \n",
    "    return (int(position / 3), position % 3) \n",
    "\n",
    "#Q learning with greedy\n",
    "#Dont necessariliy need to train with player_epsilon = 0.5\n",
    "def q_learning(env, num_episodes, discount_factor=0.99, alpha=0.05, epsilon=0.3, player_epsilon=0.5):\n",
    "    Q = defaultdict(lambda: np.zeros(9))\n",
    "    episode_lengths = np.zeros(num_episodes)\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    Turns = np.array(['X','O'])\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            episode_lengths[i_episode] += 1\n",
    "            if env.current_player == Turns[0]:\n",
    "                #move = eps_greedy(Q, str(grid), grid, epsilon=epsilon - epsilon/num_episodes*i_episode)\n",
    "                \n",
    "                #Remark: for question 2.1 epsilon needs to be constant and not decreasing\n",
    "                move = eps_greedy(Q, str(grid), grid, epsilon = epsilon)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            \n",
    "            new_grid, end, winner = env.step(move, print_grid=False)\n",
    "            \n",
    "#             if end: \n",
    "#                 Q[str(grid)][move[0] * 3 + move[1]] = env.reward(Turns[0])\n",
    "#             else: \n",
    "#             td_target = (discount_factor * np.max(Q[str(new_grid)])) * alpha \n",
    "#             td = (1 - alpha) * Q[str(grid)][move[0] * 3 + move[1]]\n",
    "#             Q[str(grid)][move[0] * 3 + move[1]] = td + td_target \n",
    "            if env.current_player == Turns[1]:\n",
    "                episode_rewards[i_episode] += env.reward(Turns[0])\n",
    "                value = Q[str(grid)][move[0] * 3 + move[1]]\n",
    "                Q[str(grid)][move[0] * 3 + move[1]] += alpha * (env.reward(Turns[0]) + discount_factor * np.max(Q[str(new_grid)]) - value)\n",
    "            grid = new_grid \n",
    "    \n",
    "    return Q, episode_lengths, episode_rewards\n",
    "\n",
    "env = TictactoeEnv()\n",
    "Q, episode_lengths, episode_rewards = q_learning(env, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uodate Q values at every move (optimal player and q-learning player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1166.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7475 0.2294000306889673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def q_eval(Q, other_player_epsilon=0.5, games=20000):\n",
    "    env.reset()\n",
    "    total_reward = []\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win = 0 \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "                possible = [int(elem) for elem in np.argsort(Q[str(grid)]) if elem in possible_moves]\n",
    "                position = possible[-1] if possible is not [] else np.random.choice(possible_moves)\n",
    "                move = (int(position / 3), position % 3) \n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            \n",
    "            #total_reward.append(env.reward(Turns[0]))\n",
    "            if end:\n",
    "                total_reward.append(env.reward(Turns[0]))\n",
    "                if(winner == Turns[0]):\n",
    "                     count_win += 1 \n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ' +  Turns[0])\n",
    "#                 print('Optimal player = ' +  Turns[1])\n",
    "#                 env.render()\n",
    "                \n",
    "                env.reset()\n",
    "                break\n",
    "        \n",
    "    return np.mean(total_reward), count_win / games \n",
    "\n",
    "\n",
    "reward, score = q_eval(Q, other_player_epsilon = 1., games = 2000)\n",
    "print(score, reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#Modifies state to specification given in project description.\n",
    "#Takes a grid and returns a 3x3x2 tensor which contains only values 0 or 1\n",
    "# such that [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent\n",
    "def modify_state_to_description(grid, my_turn_sign): \n",
    "    if('X' == my_turn_sign): #if i am first player to play\n",
    "        my_array =  np.where(grid == -1, 0, grid) #replace -1's with 0(remove other players moves)\n",
    "        other_array = np.where(grid == 1, 0, grid)  #replace 1's with 0\n",
    "        other_array = np.where(other_array == -1, 1, other_array) \n",
    "    else: #-1 = my_moves\n",
    "        my_array = np.where(grid == 1, 0, grid) #remove other players moves\n",
    "        my_array = np.where(my_array == -1, 1, my_array) #change -1 to 1's \n",
    "        other_array = np.where(grid == -1, 0, grid)\n",
    "    #print(\"grid\")#print(grid)\n",
    "    state = np.stack((my_array, other_array))\n",
    "    #print(\"state\")print(state)\n",
    "    return state\n",
    "\n",
    "#Given the state, prepares a tensor for the neural network\n",
    "def get_tensor_for_neural_net(state):\n",
    "    state = torch.from_numpy(state)\n",
    "    state = torch.flatten(state)\n",
    "    return state.unsqueeze(0).type(torch.float32) \n",
    "\n",
    "#Given integer representatin of a move, return tuple representation of move\n",
    "def get_move_from_position(position):\n",
    "    move = (int(position / 3), int(position) % 3)\n",
    "    return move\n",
    "\n",
    "def move_is_legal(move, grid):\n",
    "    possible_moves = np.where(np.ravel(grid.ravel()) == 0)[0]   \n",
    "    return move[0]*3 + move[1] in possible_moves\n",
    "\n",
    "#first player alternates each game\n",
    "def assign_players(turns, i_episode):\n",
    "    if(i_episode > 0):\n",
    "        turns = np.roll(turns, 1) \n",
    "    return turns, turns[0], turns[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Neural net from description\n",
    "# (fully connected, 2 hidden layers w 128 neurons each and relu, 3x3x2 inputs and 9 outputs)\n",
    "# (output layer uses linear activation function)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs=3*3*2, n_outputs=9):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_outputs)\n",
    "\n",
    "    # From description states are 3 ×3 × 2 tensor where\n",
    "    # where [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent(here they are flattened)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(state)\n",
    "            #print(output)\n",
    "            return output.max(1)[1].view(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNActor(): \n",
    "    \n",
    "    def __init__(self, device, batch_size, gamma, memory_size):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        \n",
    "        self.policy = Policy().to(self.device)\n",
    "        self.target = Policy().to(self.device)\n",
    "        self.target.load_state_dict(self.policy.state_dict()) #copy state from policy to target\n",
    "        self.target.eval() #put into eval state(BN, Dropout ect.. are now turned off)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=5e-4) #Description: Adam optimizer with lr = 5e-4\n",
    "\n",
    "        self.latest_loss = 0\n",
    "        \n",
    "\n",
    "    # Taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=self.device,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        #Store lastest loss for plot\n",
    "        self.latest_loss = loss.detach().numpy().item()\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    #Select action according to epsilon greedy policy.\n",
    "    def act(self, state: torch.tensor, eps_min, eps_max, n, num_exploratory_games, fixed_epsilon, fixed_eps_threshold): \n",
    "        sample = random.random()\n",
    "        \n",
    "        #Epsilon update function given in description\n",
    "        if(fixed_epsilon):\n",
    "            eps_threshold = fixed_eps_threshold\n",
    "        else: \n",
    "            eps_threshold = max(eps_min, eps_max*(1 - n/num_exploratory_games))       \n",
    "        # greedy \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return get_move_from_position(self.policy.act(state)), 0\n",
    "        else:\n",
    "            return get_move_from_position(torch.tensor([[random.randrange(9)]],dtype=torch.long)), 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to evaluate DQN against random and against optimal player\n",
    "\n",
    "#Given a DQNActor Compute Mopt and Mrand(Defined in project description)\n",
    "def compute_test_Mrand_Mopt(actor1 : DQNActor):\n",
    "    n_games = 500\n",
    "    #print(\"Mrand: \")\n",
    "    Mrand = eval_against_optimal_player(actor1, other_player_epsilon = 1., n_games=n_games)\n",
    "    #print(\"Mopt: \")\n",
    "    Mopt = eval_against_optimal_player(actor1, other_player_epsilon = 0., n_games=n_games) \n",
    "    \n",
    "    return Mrand, Mopt#Mrand, Mopt\n",
    "    \n",
    "#Evaluate DQNActor over n_games with agains Optimal player with given epsilon and output (wins-losses)/n_games\n",
    "def eval_against_optimal_player(actor1: DQNActor , other_player_epsilon, n_games = 500):\n",
    "    Turns = np.array(['X','O'])\n",
    "    was_invalid = False\n",
    "    env = TictactoeEnv()\n",
    "    losses, wins, invalid_moves, ties = 0, 0, 0 ,0\n",
    "    for i_episode in range(n_games):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns, player1, player2 = assign_players(Turns, i_episode) #first player changes every game\n",
    "        \n",
    "        other_player = OptimalPlayer(other_player_epsilon, player=player2)\n",
    "        \n",
    "        #Loop until game ends\n",
    "        for t in count():\n",
    "            #print(\"grid\")#print(grid)#print(\"state\")#print(state)\n",
    "            state = get_tensor_for_neural_net(modify_state_to_description(np.array(grid, dtype=np.float32), env.current_player))\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            if env.current_player == player1:\n",
    "                #No random moves for evaluation(eps_min, num_exploratory games ect.. not used here)\n",
    "                move, _ = actor1.act(state,  eps_min=0, eps_max=0, n=0, num_exploratory_games=0, fixed_epsilon=True, fixed_eps_threshold=0) \n",
    "            else:  \n",
    "                move = other_player.act(grid)\n",
    "\n",
    "            if move_is_legal(move, grid) == False:\n",
    "                end, was_invalid = True, True\n",
    "            else:\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "            # Observe next state\n",
    "            if not end:\n",
    "                next_state = modify_state_to_description(np.array(grid, dtype=np.float32), env.current_player)  \n",
    "                next_state = get_tensor_for_neural_net(next_state)\n",
    "            #game has ended\n",
    "            else:\n",
    "                #count wins losses\n",
    "                if was_invalid == False:\n",
    "                    if winner == player1: \n",
    "                        wins += 1 \n",
    "                    elif str(winner) == \"None\":\n",
    "                        ties += 1\n",
    "                    else:\n",
    "                        losses += 1\n",
    "                else: \n",
    "                    invalid_moves += 1 #Dont care about invalid moves for this evaluation\n",
    "                was_invalid = False #reset boolean\n",
    "                next_state = None #because finish game\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if end:\n",
    "                break\n",
    "    \n",
    "    #print(\"wins: \" + str(wins) + \", losses: \" + str(losses) + \", ties: \" + str(ties) + \", invalid moves: \" + str(invalid_moves))\n",
    "    return (wins - losses)/(n_games)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "#Training setup (default settings, this changes slightly depending on question)\n",
    "NUM_GAMES = 20000 # Description asks to train on 20k games\n",
    "BATCH_SIZE = 64     # Description sets batch size to 64\n",
    "GAMMA = 0.99        # Description sets discount factor to 0.99\n",
    "EPS_MAX = 0.8       # Description suggests this value\n",
    "EPS_MIN = 0.1       # Description suggests this value\n",
    "NUM_EXPLORATORY_GAMES = 15000 # Play with this value between [1, 40000], has big impact on how fast we learn\n",
    "TARGET_UPDATE = 500 # Description: update target_net every 500 games\n",
    "MEMORY_SIZE = 10000 # Description sets memory size to 10000\n",
    "OTHER_PLAYER_EPSILON = 0.5\n",
    "FIXED_EPS_THRESHOLD = 0.3 # Some questions ask for fixed epsilon\n",
    "FIXED_EPSILON = False     \n",
    "\n",
    "\n",
    "def train(num_games = NUM_GAMES, batch_size = BATCH_SIZE, gamma = GAMMA, eps_max = EPS_MAX, \n",
    "    eps_min = EPS_MIN, num_exploratory_games = NUM_EXPLORATORY_GAMES, target_update = TARGET_UPDATE, \n",
    "    memory_size = MEMORY_SIZE, other_player_epsilon = OTHER_PLAYER_EPSILON, \n",
    "    fixed_eps_threshold = FIXED_EPS_THRESHOLD, fixed_epsilon = FIXED_EPSILON, evaluate_M_values = False\n",
    "    ):\n",
    "\n",
    "    wins, ties, loses, invalid_moves, random_moves, total_moves_player_1 = 0, 0, 0, 0, 0, 0\n",
    "    was_invalid = False\n",
    "    Turns = np.array(['X','O'])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    actor1 = DQNActor(device = device, batch_size = batch_size, gamma = gamma, memory_size = memory_size);\n",
    "\n",
    "    env = TictactoeEnv()\n",
    "\n",
    "    #Keep track of reward and losses for plots\n",
    "    episode_durations = []\n",
    "    rewards = []\n",
    "    losses = [] \n",
    "    Mopts = []\n",
    "    Mrands = []\n",
    "\n",
    "    # Main Training loop\n",
    "    # Train over a number of games\n",
    "    for i_episode in range(num_games):\n",
    "    \n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns, player1, player2 = assign_players(Turns, i_episode) #first player changes every game #ik it works cuz if dont alternaty works rly well\n",
    "        \n",
    "        actor2 = OptimalPlayer(epsilon=other_player_epsilon, player=player2)\n",
    "        \n",
    "        #Loop until game ends\n",
    "        for t in count():\n",
    "            #print(\"grid\")#print(grid)#print(\"state\")#print(state)\n",
    "            state = get_tensor_for_neural_net(modify_state_to_description(np.array(grid, dtype=np.float32), player1))\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            if env.current_player == player1:\n",
    "                move, rand = actor1.act(state, eps_min, eps_max, i_episode, num_exploratory_games, fixed_epsilon, fixed_eps_threshold) \n",
    "                random_moves += rand\n",
    "                total_moves_player_1  += 1\n",
    "            else:  \n",
    "                move = actor2.act(grid)\n",
    "\n",
    "            #From description: if move puts us in an illegal state, we end and give reward = -1.\n",
    "            #(instead of forcing player to make legal moves)\n",
    "            if move_is_legal(move, grid) == False:\n",
    "                #print(\"not possible move\")\n",
    "                reward = -1\n",
    "                end, was_invalid = True, True\n",
    "            else:\n",
    "                #play move and get reward (Does it make sense to store also other players moves? )\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                reward = env.reward(player1) \n",
    "\n",
    "            # Observe next state\n",
    "            if not end:\n",
    "                next_state = modify_state_to_description(np.array(grid, dtype=np.float32), player1)  #probably should be player 1 here\n",
    "                next_state = get_tensor_for_neural_net(next_state)\n",
    "            #game has ended\n",
    "            else:\n",
    "                #count wins losses ties \n",
    "                if was_invalid == False:\n",
    "                    if winner == player1: \n",
    "                        wins += 1 \n",
    "                    elif str(winner) == \"None\":\n",
    "                        ties += 1\n",
    "                    else:\n",
    "                        loses += 1\n",
    "                else: \n",
    "                    invalid_moves += 1\n",
    "                was_invalid = False #reset boolean\n",
    "                next_state = None #because finish game\n",
    "        \n",
    "            #TODO if actor1 is DQNActor or actor2 is DQNActor\n",
    "            # Store the transition in memory\n",
    "       \n",
    "            action = torch.tensor([[move[0] * 3 + move[1]]]) #move\n",
    "            \n",
    "            actor1.memory.push(state, action, next_state, torch.tensor([reward]))\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network) of given actors\n",
    "            actor1.optimize_model()\n",
    "            \n",
    "            if end:\n",
    "                episode_durations.append(t + 1)\n",
    "                rewards.append(reward)\n",
    "                losses.append(actor1.latest_loss)\n",
    "                #print(losses)\n",
    "                break\n",
    "        # Update the target network(s), copying all weights and biases in DQN\n",
    "        if i_episode % target_update == 0:\n",
    "            actor1.target.load_state_dict(actor1.policy.state_dict())\n",
    "        \n",
    "        # Evaluate training\n",
    "        if (i_episode+1) % 5000 == 0 and i_episode != 1:\n",
    "            print(\"For batch of 5000 games: \" + \" Wins: \" + str(wins) + \", Losses: \" + str(loses) +\n",
    "                \", Ties: \" + str(ties) + \" Invalid moves: \" + str(invalid_moves)\n",
    "                + \", Percentage of random moves: \" + str(random_moves/total_moves_player_1))\n",
    "            wins, ties, loses, invalid_moves, random_moves, total_moves_player_1 = 0, 0, 0, 0, 0, 0 \n",
    "        \n",
    "        \n",
    "        #If want to calculate M values every 250 games\n",
    "        if(evaluate_M_values == True and (i_episode+1) % 250 == 0 and i_episode != 1):\n",
    "            Mrand, Mopt = compute_test_Mrand_Mopt(actor1)\n",
    "            Mrands.append(Mrand)\n",
    "            Mopts.append(Mopt)\n",
    "    \n",
    "    print('Complete')\n",
    "    return rewards, losses, Mrands, Mopts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 56'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=0'>1</a>\u001b[0m \u001b[39m#DEMO CELL CAN DELETE AFTER\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=1'>2</a>\u001b[0m \u001b[39m#Plots\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=2'>3</a>\u001b[0m \u001b[39m#Demo plot trains with default parameters\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=3'>4</a>\u001b[0m rewards, losses, _ , _ \u001b[39m=\u001b[39m train(other_player_epsilon\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, num_exploratory_games\u001b[39m=\u001b[39;49m \u001b[39m10000\u001b[39;49m, num_games\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m) \u001b[39m#fixed_eps_threshold=0.3, fixed_epsilon=True\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=4'>5</a>\u001b[0m \u001b[39m#Plot average reward and losses for every 250 games during training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=5'>6</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m250\u001b[39m\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 55'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_games, batch_size, gamma, eps_max, eps_min, num_exploratory_games, target_update, memory_size, other_player_epsilon, fixed_eps_threshold, fixed_epsilon, evaluate_M_values)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=101'>102</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=103'>104</a>\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network) of given actors\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=104'>105</a>\u001b[0m actor1\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=106'>107</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=107'>108</a>\u001b[0m     episode_durations\u001b[39m.\u001b[39mappend(t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 53'\u001b[0m in \u001b[0;36mDQNActor.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=65'>66</a>\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=66'>67</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=67'>68</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=69'>70</a>\u001b[0m     param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclamp_(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DEMO CELL CAN DELETE AFTER\n",
    "#Plots\n",
    "#Demo plot trains with default parameters\n",
    "rewards, losses, _ , _ = train(other_player_epsilon=1.0, num_exploratory_games= 10000, num_games=20000) #fixed_eps_threshold=0.3, fixed_epsilon=True\n",
    "#Plot average reward and losses for every 250 games during training\n",
    "n = 250\n",
    "avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "plt.plot(avg_rewards)\n",
    "#plt.xlabel('250 games') not sure what to name this, 1 unit = 250 games, maybe can scale *250 ? and call it games\n",
    "plt.ylabel('reward')\n",
    "plt.savefig('reward.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "plt.plot(avg_losses)\n",
    "#plt.xlabel('250 games')\n",
    "plt.ylabel('training loss')\n",
    "plt.savefig('loss.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 11\n",
    "#Plot average reward and average training loss for every 250 games during training. Does\n",
    "#the loss decrease? Does the agent learn to play Tic Tac Toe?\n",
    "#Expected answer: A figure with two subplots (caption length < 50 words). Specify your choice of eps.\n",
    "\n",
    "#TODO Question: should we plot them as subplot or each one their own plot then put them together in overleaf?\n",
    "thresholds = [0.05, 0.1, 0.15, 0.2, 0.3, 0.4]\n",
    "#Maybe set title = fixed_eps_threshold\n",
    "for fixed_eps_threshold in thresholds:\n",
    "    figure, axes = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "    rewards, losses, _ , _ = train(fixed_epsilon=True, fixed_eps_threshold=fixed_eps_threshold) \n",
    "    #Plot average reward and losses for every 250 games during training\n",
    "    n = 250 # Average over sets of 250 games\n",
    "    avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "    x = [i*250 for i in range(len(avg_rewards))]\n",
    "    axes[0].plot(x, avg_rewards)\n",
    "    axes[0].set_xlabel('number training games') \n",
    "    axes[0].set_ylabel('reward')\n",
    "    #axes[0].savefig('question11_reward_' + str(fixed_eps_threshold) +'.png', bbox_inches='tight')\n",
    "    #plt.show()\n",
    "    avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "    x = [i*250 for i in range(len(avg_losses))]\n",
    "    axes[1].plot(x, avg_losses)\n",
    "    axes[1].set_xlabel('number of training games')\n",
    "    axes[1].set_ylabel('training loss')\n",
    "    figure.savefig('question11_loss_' + str(fixed_eps_threshold) + '.png', bbox_inches='tight')\n",
    "    figure.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 58'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=0'>1</a>\u001b[0m \u001b[39m#TODO This question doesnt work\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=1'>2</a>\u001b[0m \u001b[39m#TODO Question 12, (apperently not as simple as setting memory_size = 1, =/ )\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=2'>3</a>\u001b[0m \u001b[39m#Repeat the training but without the replay buffer and with a batch size of 1: At every\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=5'>6</a>\u001b[0m \u001b[39m#training (caption length < 50 words).\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=6'>7</a>\u001b[0m \u001b[39m#TODO use best fixed_eps computed in Question 11\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=7'>8</a>\u001b[0m rewards, losses, _ , _ \u001b[39m=\u001b[39m train(memory_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, fixed_epsilon\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fixed_eps_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=8'>9</a>\u001b[0m \u001b[39m#Plot average reward and losses for every 250 games during training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=9'>10</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m250\u001b[39m \u001b[39m# Average over sets of 250 games\u001b[39;00m\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 55'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_games, batch_size, gamma, eps_max, eps_min, num_exploratory_games, target_update, memory_size, other_player_epsilon, fixed_eps_threshold, fixed_epsilon, evaluate_M_values)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=59'>60</a>\u001b[0m     total_moves_player_1  \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=60'>61</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=61'>62</a>\u001b[0m     move \u001b[39m=\u001b[39m actor2\u001b[39m.\u001b[39;49mact(grid)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=63'>64</a>\u001b[0m \u001b[39m#From description: if move puts us in an illegal state, we end and give reward = -1.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=64'>65</a>\u001b[0m \u001b[39m#(instead of forcing player to make legal moves)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=65'>66</a>\u001b[0m \u001b[39mif\u001b[39;00m move_is_legal(move, grid) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=66'>67</a>\u001b[0m     \u001b[39m#print(\"not possible move\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:312\u001b[0m, in \u001b[0;36mOptimalPlayer.act\u001b[0;34m(self, grid, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=309'>310</a>\u001b[0m \u001b[39m# whether move in random or not\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=310'>311</a>\u001b[0m \u001b[39mif\u001b[39;00m random\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=311'>312</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandomMove(grid)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=313'>314</a>\u001b[0m \u001b[39m# optimial policies\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=314'>315</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=315'>316</a>\u001b[0m \u001b[39m# Win\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=316'>317</a>\u001b[0m win \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwin(grid)\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:300\u001b[0m, in \u001b[0;36mOptimalPlayer.randomMove\u001b[0;34m(self, grid)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=297'>298</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandomMove\u001b[39m(\u001b[39mself\u001b[39m, grid):\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=298'>299</a>\u001b[0m     \u001b[39m\"\"\" Chose a random move from the available options. \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=299'>300</a>\u001b[0m     avail \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mempty(grid)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=301'>302</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m avail[random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(avail)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:169\u001b[0m, in \u001b[0;36mOptimalPlayer.empty\u001b[0;34m(self, grid)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=166'>167</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m9\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=167'>168</a>\u001b[0m     pos \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(i\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m), i \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=168'>169</a>\u001b[0m     \u001b[39mif\u001b[39;00m grid[pos] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=169'>170</a>\u001b[0m         avail\u001b[39m.\u001b[39mappend(pos)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=170'>171</a>\u001b[0m \u001b[39mreturn\u001b[39;00m avail\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO This question doesnt work\n",
    "#TODO Question 12, \n",
    "#Repeat the training but without the replay buffer and with a batch size of 1: At every\n",
    "#step, update the network by using only the latest transition. What do you observe?\n",
    "#Expected answer: A figure with two subplots showing average reward and average training loss during\n",
    "#training (caption length < 50 words).\n",
    "#TODO use best fixed_eps computed in Question 11\n",
    "#TODO setting batch_size = 1 breaks the algorithm =/\n",
    "rewards, losses, _ , _ = train(memory_size = 1, batch_size = 2, fixed_epsilon=True, fixed_eps_threshold=0.3) \n",
    "#Plot average reward and losses for every 250 games during training\n",
    "n = 250 # Average over sets of 250 games\n",
    "avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "x = [i*250 for i in range(len(rewards))]\n",
    "plt.plot(x, avg_rewards)\n",
    "plt.xlabel('number training games') \n",
    "plt.ylabel('reward')\n",
    "plt.savefig('question11_reward_' + str(fixed_eps_threshold) +'.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "x = [i*250 for i in range(len(avg_losses))]\n",
    "plt.plot(avg_losses)\n",
    "plt.xlabel('number of training games')\n",
    "plt.ylabel('training loss')\n",
    "plt.savefig('question11_loss_' + str(fixed_eps_threshold) + '.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For batch of 5000 games:  Wins: 255, Losses: 1723, Ties: 32 Invalid moves: 2990, Percentage of random moves: 0.745048104131296\n",
      "For batch of 5000 games:  Wins: 359, Losses: 1837, Ties: 58 Invalid moves: 2746, Percentage of random moves: 0.6521530444322545\n",
      "For batch of 5000 games:  Wins: 490, Losses: 1954, Ties: 96 Invalid moves: 2460, Percentage of random moves: 0.5473946411740945\n",
      "For batch of 5000 games:  Wins: 520, Losses: 2190, Ties: 109 Invalid moves: 2181, Percentage of random moves: 0.45633375140273286\n",
      "Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4921/278103034.py:24: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  figure.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Instead of fixing eps, use eps(n) in Equation 1. For different values of n∗, run your DQN against Opt(0.5)\n",
    "#for 20’000 games – switch the 1st player after every game. Choose several values of n∗from a reasonably\n",
    "#wide interval between 1 to 40’000 – particularly, include n∗= 1.\n",
    "\n",
    "#TODO create loop to test different n* values (num_exploratory_games)\n",
    "#Question 13\n",
    "# After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents.\n",
    "#Plot Mopt and Mrand over time. Does decreasing eps help training compared to having a fixed eps? What is\n",
    "#the effect of n∗?\n",
    "figure, axes = plt.subplots(nrows=2, ncols=1)\n",
    "#TODO test different num_eploratory_games values\n",
    "_, _, Mrands , Mopts  = train(num_games = 20000, num_exploratory_games= 20000, evaluate_M_values=True) \n",
    "#Plot average reward and losses for every 250 games during training\n",
    "n = 250 # Average over sets of 250 games\n",
    "x = [i*250 for i in range(len(Mrands))]\n",
    "axes[0].plot(x, Mrands)\n",
    "axes[0].set_xlabel('number training games') \n",
    "axes[0].set_ylabel('Mrand')\n",
    "axes[1].plot(x, Mopts)\n",
    "axes[1].set_xlabel('number of training games')\n",
    "axes[1].set_ylabel('Mopt ')\n",
    "figure.savefig('question13_Mrand_Mopt_2.png', bbox_inches='tight')\n",
    "figure.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the best value of n∗ that you found. Run DQN against Opt(eps_opt) for different values of eps_opt for\n",
    "#20’000 games – switch the 1st player after every game. Choose several values of eps_opt from a reasonably\n",
    "#wide interval between 0 to 1 – particularly, include eps_opt = 0.\n",
    "\n",
    "#Question 14. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents\n",
    "#for each value of eps_opt. Plot Mopt and Mrand over time. What do you observe? How can you explain it?\n",
    "#Expected answer: A figure showing Mopt and Mrand over time for different values of eps_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO How to evaluate invalid moves ? think maybe only need this for question 15.\n",
    "def dqn_eval(other_player_epsilon=0.5, games=20000):\n",
    "    total_reward = []\n",
    "    env.reset()\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win, none_win, loose = 0, 0, 0 \n",
    "    winner_list = [] \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        \n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                state = modify_state_to_description(np.array([[grid]], dtype=np.int32), Turns[0])\n",
    "                state = torch.from_numpy(state)\n",
    "                # Resize, and add a batch dimension (BCHW)\n",
    "                state = state.unsqueeze(0).type(torch.float32) #mmh suspicious\n",
    "                position = select_model_action(policy, state)\n",
    "                move = (int(position / 3), int(position) % 3)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            #at this point should not be making invalid moves... because player has trained\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            \n",
    "            if end:\n",
    "                if winner == Turns[0]: \n",
    "                    count_win += 1 \n",
    "                elif str(winner) == \"None\":\n",
    "                    none_win += 1\n",
    "                else:\n",
    "                    loose += 1\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ', Turns[0])\n",
    "#                 print(f'Another player with epsilon {other_player_epsilon} = ', Turns[1])\n",
    "        #             env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "    return count_win / games, none_win / games, loose / games, np.mean(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = dqn_eval(0.5, 1000)\n",
    "print(f\"Wins of DQN: {score[0]}, wins None: {score[1]}, wins Another player: {score[2]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
