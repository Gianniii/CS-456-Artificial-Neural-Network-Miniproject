{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "\n",
    "coords = lambda pos: (int(pos / 3), pos % 3) \n",
    "\n",
    "def eps_greedy(Q, state, grid, epsilon):\n",
    "    possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "    if np.random.random() < epsilon:\n",
    "        position = np.random.choice(possible_moves)\n",
    "    else:\n",
    "        possible = [int(elem) for elem in np.argsort(Q[state]) if elem in possible_moves]\n",
    "        position = possible[-1] if possible is not [] else np.random.choice(possible_moves) \n",
    "    return (int(position / 3), position % 3) \n",
    "\n",
    "#Q learning with greedy\n",
    "#Dont necessariliy need to train with player_epsilon = 0.5\n",
    "def q_learning(env, num_episodes, discount_factor=0.99, alpha=0.05, epsilon=0.3, player_epsilon=0.5):\n",
    "    Q = defaultdict(lambda: np.zeros(9))\n",
    "    episode_lengths = np.zeros(num_episodes)\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    Turns = np.array(['X','O'])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            episode_lengths[i_episode] += 1\n",
    "            if env.current_player == Turns[0]:\n",
    "                #move = eps_greedy(Q, str(grid), grid, epsilon=epsilon - epsilon/num_episodes*i_episode)\n",
    "                \n",
    "                #Remark: for question 2.1 epsilon needs to be constant and not decreasing\n",
    "                move = eps_greedy(Q, str(grid), grid, epsilon = epsilon)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            \n",
    "            new_grid, end, winner = env.step(move, print_grid=False)\n",
    "            episode_rewards[i_episode] += env.reward(env.current_player)\n",
    "#             if end: \n",
    "#                 Q[str(grid)][move[0] * 3 + move[1]] = env.reward(Turns[0])\n",
    "#             else: \n",
    "#             td_target = (discount_factor * np.max(Q[str(new_grid)])) * alpha \n",
    "#             td = (1 - alpha) * Q[str(grid)][move[0] * 3 + move[1]]\n",
    "#             Q[str(grid)][move[0] * 3 + move[1]] = td + td_target \n",
    "            if env.current_player == Turns[0]:\n",
    "                value = Q[str(grid)][move[0] * 3 + move[1]]\n",
    "                Q[str(grid)][move[0] * 3 + move[1]] += alpha * (env.reward(Turns[0]) + discount_factor * np.max(Q[str(new_grid)]) - value)\n",
    "            grid = new_grid \n",
    "    \n",
    "    return Q, episode_lengths, episode_rewards\n",
    "\n",
    "Q, episode_lengths, episode_rewards = q_learning(env, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uodate Q values at every move (optimal player and q-learning player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def q_eval(Q, other_player_epsilon=0.5, games=20000):\n",
    "    env.reset()\n",
    "    total_reward = []\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win = 0 \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "                possible = [int(elem) for elem in np.argsort(Q[str(grid)]) if elem in possible_moves]\n",
    "                position = possible[-1] if possible is not [] else np.random.choice(possible_moves)\n",
    "                move = (int(position / 3), position % 3) \n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            if winner == Turns[0]: \n",
    "                count_win += 1 \n",
    "            if end:\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ' +  Turns[0])\n",
    "#                 print('Optimal player = ' +  Turns[1])\n",
    "#                 env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "        \n",
    "    return np.mean(total_reward), count_win / games \n",
    "\n",
    "\n",
    "reward, score = q_eval(Q, other_player_epsilon = 1., games = 2000)\n",
    "print(score, reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#Modifies state to specification given in project description.\n",
    "#Takes a grid and returns a 3x3x2 tensor which contains only values 0 or 1\n",
    "# such that [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent\n",
    "def modify_state_to_description(grid, my_turn_sign): \n",
    "    if('X' == my_turn_sign): #if i am first player to play\n",
    "        my_array =  np.where(grid == -1, 0, grid) #replace -1's with 0(remove other players moves)\n",
    "        other_array = np.where(grid == 1, 0, grid)  #replace 1's with 0\n",
    "        other_array = np.where(other_array == -1, 1, other_array) \n",
    "    else: #-1 = my_moves\n",
    "        my_array = np.where(grid == 1, 0, grid) #remove other players moves\n",
    "        my_array = np.where(my_array == -1, 1, my_array) #change -1 to 1's \n",
    "        other_array = np.where(grid == -1, 0, grid)\n",
    "    #print(\"grid\")#print(grid)\n",
    "    state = np.stack((my_array, other_array))\n",
    "    #print(\"state\")print(state)\n",
    "    return state\n",
    "\n",
    "#Given the state, prepares a tensor for the neural network\n",
    "def get_tensor_for_neural_net(state):\n",
    "    state = torch.from_numpy(state)\n",
    "    state = torch.flatten(state)\n",
    "    return state.unsqueeze(0).type(torch.float32) \n",
    "\n",
    "#Given integer representatin of a move, return tuple representation of move\n",
    "def get_move_from_position(position):\n",
    "    move = (int(position / 3), int(position) % 3)\n",
    "    return move\n",
    "\n",
    "def move_is_legal(move, grid):\n",
    "    possible_moves = np.where(np.ravel(grid.ravel()) == 0)[0]   \n",
    "    return move[0]*3 + move[1] in possible_moves\n",
    "\n",
    "#first player alternates each game\n",
    "def assign_players(turns, i_episode):\n",
    "    if(i_episode > 0):\n",
    "        turns = np.roll(turns, 1) \n",
    "    return turns[0], turns[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Neural net from description\n",
    "# (fully connected, 2 hidden layers w 128 neurons each and relu, 3x3x2 inputs and 9 outputs)\n",
    "# (output layer uses linear activation function)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs=3*3*2, n_outputs=9):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_outputs)\n",
    "\n",
    "    # From description states are 3 ×3 × 2 tensor where\n",
    "    # where [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent(here they are flattened)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(state)\n",
    "            #print(output)\n",
    "            return output.max(1)[1].view(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNActor(): \n",
    "    \n",
    "    def __init__(self, device, batch_size, gamma, memory_size):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        \n",
    "        self.policy = Policy().to(self.device)\n",
    "        self.target = Policy().to(self.device)\n",
    "        self.target.load_state_dict(self.policy.state_dict()) #copy state from policy to target\n",
    "        self.target.eval() #put into eval state(BN, Dropout ect.. are now turned off)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=5e-4) #Description: Adam optimizer with lr = 5e-4\n",
    "\n",
    "        self.latest_loss = 0\n",
    "        \n",
    "\n",
    "    # Taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=self.device,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        #Store lastest loss for plot\n",
    "        self.latest_loss = loss.detach().numpy().item()\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    #Select action according to epsilon greedy policy.\n",
    "    def act(self, state: torch.tensor, eps_min, eps_max, n, num_exploratory_games, fixed_epsilon, fixed_eps_threshold): \n",
    "        sample = random.random()\n",
    "        \n",
    "        #Epsilon update function given in description\n",
    "        if(fixed_epsilon):\n",
    "            eps_threshold = fixed_eps_threshold\n",
    "        else: \n",
    "            eps_threshold = max(eps_min, eps_max*(1 - n/num_exploratory_games))       \n",
    "        # greedy \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return get_move_from_position(self.policy.act(state)), 0\n",
    "        else:\n",
    "            return get_move_from_position(torch.tensor([[random.randrange(9)]],dtype=torch.long)), 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Create bool for fixed epsilon.\n",
    "#TODO Start making official plots for the damn questions(gonna suck testing different n, epsilon value and making plots for all)\n",
    "\n",
    "#Training setup (default settings, this changes slightly depending on question)\n",
    "NUM_GAMES = 20000  # Description asks to train on 20k games\n",
    "BATCH_SIZE = 64     # Description sets batch size to 64\n",
    "GAMMA = 0.99        # Description sets discount factor to 0.99\n",
    "EPS_MAX = 0.8       # Description suggests this value\n",
    "EPS_MIN = 0.1       # Description suggests this value\n",
    "NUM_EXPLORATORY_GAMES = 15000 # Play with this value between [1, 40000], has big impact on how fast we learn\n",
    "TARGET_UPDATE = 500 # Description: update target_net every 500 games\n",
    "MEMORY_SIZE = 10000 # Description sets memory size to 10000\n",
    "OTHER_PLAYER_EPSILON = 0.5\n",
    "FIXED_EPS_THRESHOLD = 0.3 # Some questions ask for fixed epsilon\n",
    "FIXED_EPSILON = False     \n",
    "\n",
    "\n",
    "def train(num_games = NUM_GAMES, batch_size = BATCH_SIZE, gamma = GAMMA, eps_max = EPS_MAX, \n",
    "    eps_min = EPS_MIN, num_exploratory_games = NUM_EXPLORATORY_GAMES, target_update = TARGET_UPDATE, \n",
    "    memory_size = MEMORY_SIZE, other_player_epsilon = OTHER_PLAYER_EPSILON, \n",
    "    fixed_eps_threshold = FIXED_EPS_THRESHOLD, fixed_epsilon = FIXED_EPSILON, \n",
    "    ):\n",
    "\n",
    "    wins, ties, loses, invalid_moves, random_moves, total_moves_player_1 = 0, 0, 0, 0, 0, 0\n",
    "    was_invalid = False\n",
    "    Turns = np.array(['X','O'])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    actor1 = DQNActor(device = device, batch_size = batch_size, gamma = gamma, memory_size = memory_size);\n",
    "\n",
    "    env = TictactoeEnv()\n",
    "\n",
    "    #Keep track of reward and losses for plots\n",
    "    episode_durations = []\n",
    "    rewards = []\n",
    "    losses = [] \n",
    "\n",
    "    # Main Training loop\n",
    "    # Train over a number of games\n",
    "    for i_episode in range(num_games):\n",
    "    \n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        player1, player2 = assign_players(Turns, i_episode) #first player changes every game\n",
    "        \n",
    "        actor2 = OptimalPlayer(epsilon=other_player_epsilon, player=player2)\n",
    "        \n",
    "        #Loop until game ends\n",
    "        for t in count():\n",
    "            #print(\"grid\")#print(grid)#print(\"state\")#print(state)\n",
    "            state = get_tensor_for_neural_net(modify_state_to_description(np.array(grid, dtype=np.float32), env.current_player))\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            if env.current_player == player1:\n",
    "                move, rand = actor1.act(state, eps_min, eps_max, i_episode, num_exploratory_games, fixed_epsilon, fixed_eps_threshold) \n",
    "                random_moves += rand\n",
    "                total_moves_player_1  += 1\n",
    "            else:  \n",
    "                move = actor2.act(grid)\n",
    "\n",
    "            #From description: if move puts us in an illegal state, we end and give reward = -1.\n",
    "            #(instead of forcing player to make legal moves)\n",
    "            if move_is_legal(move, grid) == False:\n",
    "                #print(\"not possible move\")\n",
    "                reward = -1\n",
    "                end, was_invalid = True, True\n",
    "            else:\n",
    "                #play move and get reward (Does it make sense to store also other players moves? )\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                reward = env.reward(Turns[0]) \n",
    "\n",
    "            # Observe next state\n",
    "            if not end:\n",
    "                next_state = modify_state_to_description(np.array(grid, dtype=np.float32), env.current_player)  \n",
    "                next_state = get_tensor_for_neural_net(next_state)\n",
    "            #game has ended\n",
    "            else:\n",
    "                #count wins losses ties \n",
    "                if was_invalid == False:\n",
    "                    if winner == Turns[0]: \n",
    "                        wins += 1 \n",
    "                    elif str(winner) == \"None\":\n",
    "                        ties += 1\n",
    "                    else:\n",
    "                        loses += 1\n",
    "                else: \n",
    "                    invalid_moves += 1\n",
    "                was_invalid = False #reset boolean\n",
    "                next_state = None #because finish game\n",
    "        \n",
    "            #TODO if actor1 is DQNActor or actor2 is DQNActor\n",
    "            # Store the transition in memory\n",
    "            action = torch.tensor([[move[0] * 3 + move[1]]]) #move\n",
    "            actor1.memory.push(state, action, next_state, torch.tensor([reward]))\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network) of given actors\n",
    "            actor1.optimize_model()\n",
    "            \n",
    "            if end:\n",
    "                episode_durations.append(t + 1)\n",
    "                rewards.append(reward)\n",
    "                losses.append(actor1.latest_loss)\n",
    "                #print(losses)\n",
    "                break\n",
    "        # Update the target network(s), copying all weights and biases in DQN\n",
    "        if i_episode % target_update == 0:\n",
    "            actor1.target.load_state_dict(actor1.policy.state_dict())\n",
    "        \n",
    "        # Evaluate training\n",
    "        if (i_episode+1) % 5000 == 0 and i_episode != 1:\n",
    "            print(\"For batch of 5000 games: \" + \" Wins: \" + str(wins) + \", Loses: \" + str(loses) +\n",
    "                \", Ties: \" + str(ties) + \" Invalid moves: \" + str(invalid_moves)\n",
    "                + \", Percentage of random moves: \" + str(random_moves/total_moves_player_1))\n",
    "            wins, ties, loses, invalid_moves, random_moves, total_moves_player_1 = 0, 0, 0, 0, 0, 0 \n",
    "    print('Complete')\n",
    "    return rewards, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 55'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000062?line=0'>1</a>\u001b[0m \u001b[39m#DEMO CELL CAN DELETE AFTER\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000062?line=1'>2</a>\u001b[0m \u001b[39m#Plots\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000062?line=2'>3</a>\u001b[0m \u001b[39m#Demo plot trains with default parameters\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000062?line=3'>4</a>\u001b[0m rewards, losses \u001b[39m=\u001b[39m train() \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000062?line=4'>5</a>\u001b[0m \u001b[39m#Plot average reward and losses for every 250 games during training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000062?line=5'>6</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m250\u001b[39m\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 54'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_games, batch_size, gamma, eps_max, eps_min, num_exploratory_games, target_update, memory_size, other_player_epsilon, fixed_eps_threshold, fixed_epsilon)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=98'>99</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=100'>101</a>\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network) of given actors\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=101'>102</a>\u001b[0m actor1\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=103'>104</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=104'>105</a>\u001b[0m     episode_durations\u001b[39m.\u001b[39mappend(t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 53'\u001b[0m in \u001b[0;36mDQNActor.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=65'>66</a>\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=66'>67</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=67'>68</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=69'>70</a>\u001b[0m     param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclamp_(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DEMO CELL CAN DELETE AFTER\n",
    "#Plots\n",
    "#Demo plot trains with default parameters\n",
    "rewards, losses = train() \n",
    "#Plot average reward and losses for every 250 games during training\n",
    "n = 250\n",
    "avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "plt.plot(avg_rewards)\n",
    "#plt.xlabel('250 games') not sure what to name this, 1 unit = 250 games, maybe can scale *250 ? and call it games\n",
    "plt.ylabel('reward')\n",
    "plt.title('MyFirstEverPlotPoggerss')\n",
    "plt.savefig('foo.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "plt.plot(avg_losses)\n",
    "#plt.xlabel('250 games')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('MyFirstEverPlotPoggerss')\n",
    "plt.savefig('foo.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For batch of 5000 games:  Wins: 4165, Loses: 153, Ties: 154 Invalid moves: 528, Percentage of random moves: 0.10074816590397327\n",
      "For batch of 5000 games:  Wins: 4346, Loses: 79, Ties: 115 Invalid moves: 460, Percentage of random moves: 0.10080731585339715\n",
      "For batch of 5000 games:  Wins: 4288, Loses: 63, Ties: 155 Invalid moves: 494, Percentage of random moves: 0.09834089619965967\n",
      "For batch of 5000 games:  Wins: 4258, Loses: 72, Ties: 170 Invalid moves: 500, Percentage of random moves: 0.09971310615072423\n",
      "Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7135/1352073973.py:26: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  figure.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For batch of 5000 games:  Wins: 3709, Loses: 166, Ties: 172 Invalid moves: 953, Percentage of random moves: 0.19932357914859203\n",
      "For batch of 5000 games:  Wins: 3771, Loses: 110, Ties: 165 Invalid moves: 954, Percentage of random moves: 0.20322859613721533\n",
      "For batch of 5000 games:  Wins: 3807, Loses: 92, Ties: 150 Invalid moves: 951, Percentage of random moves: 0.19879474333841574\n",
      "For batch of 5000 games:  Wins: 3819, Loses: 65, Ties: 177 Invalid moves: 939, Percentage of random moves: 0.20226490825688073\n",
      "Complete\n",
      "For batch of 5000 games:  Wins: 3371, Loses: 177, Ties: 127 Invalid moves: 1325, Percentage of random moves: 0.302170158444094\n",
      "For batch of 5000 games:  Wins: 3422, Loses: 118, Ties: 125 Invalid moves: 1335, Percentage of random moves: 0.3033168353401788\n",
      "For batch of 5000 games:  Wins: 3424, Loses: 88, Ties: 107 Invalid moves: 1381, Percentage of random moves: 0.3084562142642191\n",
      "For batch of 5000 games:  Wins: 3461, Loses: 81, Ties: 136 Invalid moves: 1322, Percentage of random moves: 0.29274975931274533\n",
      "Complete\n",
      "For batch of 5000 games:  Wins: 3049, Loses: 170, Ties: 110 Invalid moves: 1671, Percentage of random moves: 0.39791218913110227\n",
      "For batch of 5000 games:  Wins: 3124, Loses: 101, Ties: 100 Invalid moves: 1675, Percentage of random moves: 0.4088221919054115\n",
      "For batch of 5000 games:  Wins: 3111, Loses: 75, Ties: 125 Invalid moves: 1689, Percentage of random moves: 0.3998642226748133\n"
     ]
    }
   ],
   "source": [
    "#Question 11\n",
    "#Plot average reward and average training loss for every 250 games during training. Does\n",
    "#the loss decrease? Does the agent learn to play Tic Tac Toe?\n",
    "#Expected answer: A figure with two subplots (caption length < 50 words). Specify your choice of eps.\n",
    "\n",
    "#TODO Question: should we plot them as subplot or each one their own plot?\n",
    "thresholds = [0.05, 0.1, 0.15, 0.2, 0.3, 0.4]\n",
    "#Maybe set title = fixed_eps_threshold\n",
    "for fixed_eps_threshold in thresholds:\n",
    "    figure, axes = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "    rewards, losses = train(fixed_epsilon=True, fixed_eps_threshold=fixed_eps_threshold) \n",
    "    #Plot average reward and losses for every 250 games during training\n",
    "    n = 250 # Average over sets of 250 games\n",
    "    avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "    x = [i*250 for i in range(len(avg_rewards))]\n",
    "    axes[0].plot(x, avg_rewards)\n",
    "    axes[0].set_xlabel('number training games') \n",
    "    axes[0].set_ylabel('reward')\n",
    "    #axes[0].savefig('question11_reward_' + str(fixed_eps_threshold) +'.png', bbox_inches='tight')\n",
    "    #plt.show()\n",
    "    avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "    x = [i*250 for i in range(len(avg_losses))]\n",
    "    axes[1].plot(x, avg_losses)\n",
    "    axes[1].set_xlabel('number of training games')\n",
    "    axes[1].set_ylabel('training loss')\n",
    "    figure.savefig('question11_loss_' + str(fixed_eps_threshold) + '.png', bbox_inches='tight')\n",
    "    figure.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors, or that you (the operator writer) forgot to register a fallback function.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:21063 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:29726 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1258 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11220 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 57'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=0'>1</a>\u001b[0m \u001b[39m#Question 12\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=1'>2</a>\u001b[0m \u001b[39m#Repeat the training but without the replay buffer and with a batch size of 1: At every\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=2'>3</a>\u001b[0m \u001b[39m#step, update the network by using only the latest transition. What do you observe?\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=3'>4</a>\u001b[0m \u001b[39m#Expected answer: A figure with two subplots showing average reward and average training loss during\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=4'>5</a>\u001b[0m \u001b[39m#training (caption length < 50 words).\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=5'>6</a>\u001b[0m \u001b[39m#TODO verify this is correct\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=6'>7</a>\u001b[0m rewards, losses \u001b[39m=\u001b[39m train(memory_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, fixed_epsilon\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fixed_eps_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=7'>8</a>\u001b[0m \u001b[39m#Plot average reward and losses for every 250 games during training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000065?line=8'>9</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m250\u001b[39m \u001b[39m# Average over sets of 250 games\u001b[39;00m\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 54'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_games, batch_size, gamma, eps_max, eps_min, num_exploratory_games, target_update, memory_size, other_player_epsilon, fixed_eps_threshold, fixed_epsilon)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=98'>99</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=100'>101</a>\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network) of given actors\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=101'>102</a>\u001b[0m actor1\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=103'>104</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=104'>105</a>\u001b[0m     episode_durations\u001b[39m.\u001b[39mappend(t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 53'\u001b[0m in \u001b[0;36mDQNActor.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=29'>30</a>\u001b[0m \u001b[39m# Compute a mask of non-final states and concatenate the batch elements\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=30'>31</a>\u001b[0m \u001b[39m# (a final state would've been the one after which simulation ended)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=31'>32</a>\u001b[0m non_final_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=32'>33</a>\u001b[0m     \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m s: s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, batch\u001b[39m.\u001b[39mnext_state)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=33'>34</a>\u001b[0m     device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=34'>35</a>\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbool,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=35'>36</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=36'>37</a>\u001b[0m non_final_next_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=37'>38</a>\u001b[0m     [s \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m batch\u001b[39m.\u001b[39;49mnext_state \u001b[39mif\u001b[39;49;00m s \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=38'>39</a>\u001b[0m state_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(batch\u001b[39m.\u001b[39mstate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000053?line=39'>40</a>\u001b[0m action_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(batch\u001b[39m.\u001b[39maction)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors, or that you (the operator writer) forgot to register a fallback function.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:21063 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:29726 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1258 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11220 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "#TODO Question 12, (apperently not as simple as setting memory_size = 1, =/ )\n",
    "#Repeat the training but without the replay buffer and with a batch size of 1: At every\n",
    "#step, update the network by using only the latest transition. What do you observe?\n",
    "#Expected answer: A figure with two subplots showing average reward and average training loss during\n",
    "#training (caption length < 50 words).\n",
    "#\n",
    "rewards, losses = train(memory_size = 1, batch_size = 1, fixed_epsilon=True, fixed_eps_threshold=0.3) \n",
    "#Plot average reward and losses for every 250 games during training\n",
    "n = 250 # Average over sets of 250 games\n",
    "avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "x = [i*250 for i in range(len(rewards))]\n",
    "plt.plot(x, avg_rewards)\n",
    "plt.xlabel('number training games') \n",
    "plt.ylabel('reward')\n",
    "plt.savefig('question11_reward_' + str(fixed_eps_threshold) +'.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "x = [i*250 for i in range(len(avg_losses))]\n",
    "plt.plot(avg_losses)\n",
    "plt.xlabel('number of training games')\n",
    "plt.ylabel('training loss')\n",
    "plt.savefig('question11_loss_' + str(fixed_eps_threshold) + '.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of fixing eps, use eps(n) in Equation 1. For different values of n∗, run your DQN against Opt(0.5)\n",
    "#for 20’000 games – switch the 1st player after every game. Choose several values of n∗from a reasonably\n",
    "#wide interval between 1 to 40’000 – particularly, include n∗= 1.\n",
    "#TODO boolean within train to compute and record Mopt and Mrand every 250 games \n",
    "#Question 13\n",
    "# After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents.\n",
    "#Plot Mopt and Mrand over time. Does decreasing eps help training compared to having a fixed eps? What is\n",
    "#the effect of n∗?\n",
    "#Expected answer: A figure showing Mopt and Mrand over time for different values of speeds of n∗(caption\n",
    "#length < 250 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the best value of n∗ that you found. Run DQN against Opt(eps_opt) for different values of eps_opt for\n",
    "#20’000 games – switch the 1st player after every game. Choose several values of eps_opt from a reasonably\n",
    "#wide interval between 0 to 1 – particularly, include eps_opt = 0.\n",
    "\n",
    "#Question 14. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents\n",
    "#for each value of eps_opt. Plot Mopt and Mrand over time. What do you observe? How can you explain it?\n",
    "#Expected answer: A figure showing Mopt and Mrand over time for different values of eps_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO How to evaluate invalid moves ? think maybe only need this for question 15.\n",
    "def dqn_eval(other_player_epsilon=0.5, games=20000):\n",
    "    total_reward = []\n",
    "    env.reset()\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win, none_win, loose = 0, 0, 0 \n",
    "    winner_list = [] \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        \n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                state = modify_state_to_description(np.array([[grid]], dtype=np.int32), Turns[0])\n",
    "                state = torch.from_numpy(state)\n",
    "                # Resize, and add a batch dimension (BCHW)\n",
    "                state = state.unsqueeze(0).type(torch.float32) #mmh suspicious\n",
    "                position = select_model_action(policy, state)\n",
    "                move = (int(position / 3), int(position) % 3)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            #at this point should not be making invalid moves... because player has trained\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            \n",
    "            if end:\n",
    "                if winner == Turns[0]: \n",
    "                    count_win += 1 \n",
    "                elif str(winner) == \"None\":\n",
    "                    none_win += 1\n",
    "                else:\n",
    "                    loose += 1\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ', Turns[0])\n",
    "#                 print(f'Another player with epsilon {other_player_epsilon} = ', Turns[1])\n",
    "        #             env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "    return count_win / games, none_win / games, loose / games, np.mean(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "There is already a chess on position (0, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 60'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000056?line=0'>1</a>\u001b[0m score \u001b[39m=\u001b[39m dqn_eval(\u001b[39m1.\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000056?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWins of DQN: \u001b[39m\u001b[39m{\u001b[39;00mscore[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, wins None: \u001b[39m\u001b[39m{\u001b[39;00mscore[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, wins Another player: \u001b[39m\u001b[39m{\u001b[39;00mscore[\u001b[39m2\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 59'\u001b[0m in \u001b[0;36mdqn_eval\u001b[0;34m(other_player_epsilon, games)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=19'>20</a>\u001b[0m     move \u001b[39m=\u001b[39m player\u001b[39m.\u001b[39mact(grid)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=20'>21</a>\u001b[0m \u001b[39m#at this point should not be making invalid moves... because player has trained\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=21'>22</a>\u001b[0m grid, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(move, print_grid\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=22'>23</a>\u001b[0m total_reward\u001b[39m.\u001b[39mappend(env\u001b[39m.\u001b[39mreward(env\u001b[39m.\u001b[39mcurrent_player))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:61\u001b[0m, in \u001b[0;36mTictactoeEnv.step\u001b[0;34m(self, position, print_grid)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=58'>59</a>\u001b[0m     position \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(position)\n\u001b[1;32m     <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid[position] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=60'>61</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThere is already a chess on position \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(position))\n\u001b[1;32m     <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=62'>63</a>\u001b[0m \u001b[39m# place a chess on the position\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=63'>64</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid[position] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer2value[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_player]\n",
      "\u001b[0;31mValueError\u001b[0m: There is already a chess on position (0, 1)."
     ]
    }
   ],
   "source": [
    "score = dqn_eval(0.5, 1000)\n",
    "print(f\"Wins of DQN: {score[0]}, wins None: {score[1]}, wins Another player: {score[2]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
