{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000043?line=48'>49</a>\u001b[0m             grid \u001b[39m=\u001b[39m new_grid \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000043?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Q, episode_lengths, episode_rewards\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000043?line=52'>53</a>\u001b[0m Q, episode_lengths, episode_rewards \u001b[39m=\u001b[39m q_learning(env, \u001b[39m20000\u001b[39;49m)\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 44'\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, num_episodes, discount_factor, alpha, epsilon, player_epsilon)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000043?line=45'>46</a>\u001b[0m         \u001b[39mif\u001b[39;00m env\u001b[39m.\u001b[39mcurrent_player \u001b[39m==\u001b[39m Turns[\u001b[39m0\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000043?line=46'>47</a>\u001b[0m             value \u001b[39m=\u001b[39m Q[\u001b[39mstr\u001b[39m(grid)][move[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m \u001b[39m+\u001b[39m move[\u001b[39m1\u001b[39m]]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000043?line=47'>48</a>\u001b[0m             Q[\u001b[39mstr\u001b[39m(grid)][move[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m \u001b[39m+\u001b[39m move[\u001b[39m1\u001b[39m]] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39m (env\u001b[39m.\u001b[39mreward(Turns[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m discount_factor \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(Q[\u001b[39mstr\u001b[39;49m(new_grid)]) \u001b[39m-\u001b[39m value)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000043?line=48'>49</a>\u001b[0m         grid \u001b[39m=\u001b[39m new_grid \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000043?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m Q, episode_lengths, episode_rewards\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py:1592\u001b[0m, in \u001b[0;36m_array_str_implementation\u001b[0;34m(a, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=1585'>1586</a>\u001b[0m \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():\n\u001b[1;32m   <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=1586'>1587</a>\u001b[0m     \u001b[39m# obtain a scalar and call str on it, avoiding problems for subclasses\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=1587'>1588</a>\u001b[0m     \u001b[39m# for which indexing with () returns a 0d instead of a scalar by using\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=1588'>1589</a>\u001b[0m     \u001b[39m# ndarray's getindex. Also guard against recursive 0d object arrays.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=1589'>1590</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _guarded_repr_or_str(np\u001b[39m.\u001b[39mndarray\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(a, ()))\n\u001b[0;32m-> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=1591'>1592</a>\u001b[0m \u001b[39mreturn\u001b[39;00m array2string(a, max_line_width, precision, suppress_small, \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py:736\u001b[0m, in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=732'>733</a>\u001b[0m \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=733'>734</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m[]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=735'>736</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _array2string(a, options, separator, prefix)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py:513\u001b[0m, in \u001b[0;36m_recursive_guard.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=510'>511</a>\u001b[0m repr_running\u001b[39m.\u001b[39madd(key)\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=511'>512</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=512'>513</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=513'>514</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=514'>515</a>\u001b[0m     repr_running\u001b[39m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py:539\u001b[0m, in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=535'>536</a>\u001b[0m     summary_insert \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=537'>538</a>\u001b[0m \u001b[39m# find the right formatting function for the array\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=538'>539</a>\u001b[0m format_function \u001b[39m=\u001b[39m _get_format_function(data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=540'>541</a>\u001b[0m \u001b[39m# skip over \"[\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=541'>542</a>\u001b[0m next_line_prefix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py:472\u001b[0m, in \u001b[0;36m_get_format_function\u001b[0;34m(data, **options)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=469'>470</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m formatdict[\u001b[39m'\u001b[39m\u001b[39mlongfloat\u001b[39m\u001b[39m'\u001b[39m]()\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=470'>471</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=471'>472</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m formatdict[\u001b[39m'\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m'\u001b[39;49m]()\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=472'>473</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39missubclass\u001b[39m(dtypeobj, _nt\u001b[39m.\u001b[39mcomplexfloating):\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=473'>474</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(dtypeobj, _nt\u001b[39m.\u001b[39mclongfloat):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py:411\u001b[0m, in \u001b[0;36m_get_formatdict.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=402'>403</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_formatdict\u001b[39m(data, \u001b[39m*\u001b[39m, precision, floatmode, suppress, sign, legacy,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=403'>404</a>\u001b[0m                     formatter, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=404'>405</a>\u001b[0m     \u001b[39m# note: extra arguments in kwargs are ignored\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=405'>406</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=406'>407</a>\u001b[0m     \u001b[39m# wrapped in lambdas to avoid taking a code path with the wrong type of data\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=407'>408</a>\u001b[0m     formatdict \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=408'>409</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbool\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: BoolFormat(data),\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=409'>410</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: IntegerFormat(data),\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=410'>411</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: FloatingFormat(\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=411'>412</a>\u001b[0m             data, precision, floatmode, suppress, sign, legacy\u001b[39m=\u001b[39;49mlegacy),\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=412'>413</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlongfloat\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: FloatingFormat(\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=413'>414</a>\u001b[0m             data, precision, floatmode, suppress, sign, legacy\u001b[39m=\u001b[39mlegacy),\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=414'>415</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcomplexfloat\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: ComplexFloatingFormat(\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=415'>416</a>\u001b[0m             data, precision, floatmode, suppress, sign, legacy\u001b[39m=\u001b[39mlegacy),\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=416'>417</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlongcomplexfloat\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: ComplexFloatingFormat(\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=417'>418</a>\u001b[0m             data, precision, floatmode, suppress, sign, legacy\u001b[39m=\u001b[39mlegacy),\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=418'>419</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: DatetimeFormat(data, legacy\u001b[39m=\u001b[39mlegacy),\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=419'>420</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtimedelta\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: TimedeltaFormat(data),\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=420'>421</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: _object_format,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=421'>422</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mvoid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: str_format,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=422'>423</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnumpystr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlambda\u001b[39;00m: repr_format}\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=424'>425</a>\u001b[0m     \u001b[39m# we need to wrap values in `formatter` in a lambda, so that the interface\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=425'>426</a>\u001b[0m     \u001b[39m# is the same as the above values.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=426'>427</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mindirect\u001b[39m(x):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py:932\u001b[0m, in \u001b[0;36mFloatingFormat.__init__\u001b[0;34m(self, data, precision, floatmode, suppress_small, sign, legacy)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=928'>929</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexp_format \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=929'>930</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlarge_exponent \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=931'>932</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfillFormat(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py:939\u001b[0m, in \u001b[0;36mFloatingFormat.fillFormat\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=935'>936</a>\u001b[0m finite_vals \u001b[39m=\u001b[39m data[isfinite(data)]\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=937'>938</a>\u001b[0m \u001b[39m# choose exponential mode based on the non-zero finite values:\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=938'>939</a>\u001b[0m abs_non_zero \u001b[39m=\u001b[39m absolute(finite_vals[finite_vals \u001b[39m!=\u001b[39;49m \u001b[39m0\u001b[39;49m])\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=939'>940</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(abs_non_zero) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/numpy/core/arrayprint.py?line=940'>941</a>\u001b[0m     max_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(abs_non_zero)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "\n",
    "coords = lambda pos: (int(pos / 3), pos % 3) \n",
    "\n",
    "def eps_greedy(Q, state, grid, epsilon):\n",
    "    possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "    if np.random.random() < epsilon:\n",
    "        position = np.random.choice(possible_moves)\n",
    "    else:\n",
    "        possible = [int(elem) for elem in np.argsort(Q[state]) if elem in possible_moves]\n",
    "        position = possible[-1] if possible is not [] else np.random.choice(possible_moves) \n",
    "    return (int(position / 3), position % 3) \n",
    "\n",
    "#Q learning with greedy\n",
    "#Dont necessariliy need to train with player_epsilon = 0.5\n",
    "def q_learning(env, num_episodes, discount_factor=0.99, alpha=0.05, epsilon=0.3, player_epsilon=0.5):\n",
    "    Q = defaultdict(lambda: np.zeros(9))\n",
    "    episode_lengths = np.zeros(num_episodes)\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    Turns = np.array(['X','O'])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            episode_lengths[i_episode] += 1\n",
    "            if env.current_player == Turns[0]:\n",
    "                #move = eps_greedy(Q, str(grid), grid, epsilon=epsilon - epsilon/num_episodes*i_episode)\n",
    "                \n",
    "                #Remark: for question 2.1 epsilon needs to be constant and not decreasing\n",
    "                move = eps_greedy(Q, str(grid), grid, epsilon = epsilon)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            \n",
    "            new_grid, end, winner = env.step(move, print_grid=False)\n",
    "            episode_rewards[i_episode] += env.reward(env.current_player)\n",
    "#             if end: \n",
    "#                 Q[str(grid)][move[0] * 3 + move[1]] = env.reward(Turns[0])\n",
    "#             else: \n",
    "#             td_target = (discount_factor * np.max(Q[str(new_grid)])) * alpha \n",
    "#             td = (1 - alpha) * Q[str(grid)][move[0] * 3 + move[1]]\n",
    "#             Q[str(grid)][move[0] * 3 + move[1]] = td + td_target \n",
    "            if env.current_player == Turns[0]:\n",
    "                value = Q[str(grid)][move[0] * 3 + move[1]]\n",
    "                Q[str(grid)][move[0] * 3 + move[1]] += alpha * (env.reward(Turns[0]) + discount_factor * np.max(Q[str(new_grid)]) - value)\n",
    "            grid = new_grid \n",
    "    \n",
    "    return Q, episode_lengths, episode_rewards\n",
    "\n",
    "Q, episode_lengths, episode_rewards = q_learning(env, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uodate Q values at every move (optimal player and q-learning player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def q_eval(Q, other_player_epsilon=0.5, games=20000):\n",
    "    env.reset()\n",
    "    total_reward = []\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win = 0 \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "                possible = [int(elem) for elem in np.argsort(Q[str(grid)]) if elem in possible_moves]\n",
    "                position = possible[-1] if possible is not [] else np.random.choice(possible_moves)\n",
    "                move = (int(position / 3), position % 3) \n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            if winner == Turns[0]: \n",
    "                count_win += 1 \n",
    "            if end:\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ' +  Turns[0])\n",
    "#                 print('Optimal player = ' +  Turns[1])\n",
    "#                 env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "        \n",
    "    return np.mean(total_reward), count_win / games \n",
    "\n",
    "\n",
    "reward, score = q_eval(Q, other_player_epsilon = 1., games = 2000)\n",
    "print(score, reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = TictactoeEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "#This seems better because its limited by a max_len=capacity, but dont know it changes much\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay memory\n",
    "class DataSampler(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = {'state':[], 'action':[], 'next_state':[], 'reward':[]}\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory['state'].append(state)\n",
    "        self.memory['action'].append(action)\n",
    "        self.memory['next_state'].append(next_state)\n",
    "        self.memory['reward'].append(reward)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return {key: random.sample(self.memory[key], batch_size) for key in self.memory.keys()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural net\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, states, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=1, stride=1)\n",
    "        self.conv3 = nn.Conv1d(32, 32, kernel_size=1, stride=1)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 1, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        conv_s = conv2d_size_out(conv2d_size_out(conv2d_size_out(states)))\n",
    "        linear_input_size = conv_s #* 32\n",
    "        self.head = nn.Linear(32, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, 1)\n",
    "        x = x.type(torch.float32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural net from description \n",
    "# (fully connected, 2 hidden layers w 128 neurons and relu, 3x3x2 inputs and 9 outputs)\n",
    "class DQN_FNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #From description states are 3 ×3 × 2 tensor where\n",
    "        #where [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent\n",
    "        self.fc1 = nn.Linear(3*3*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1, 3*3*2)))\n",
    "        x = F.relu(self.fc2(x.view(-1, 128)))\n",
    "        x = self.fc3(x.view(-1, 128)) #should i do (-1, 128) or (128, -1)?\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training setup\n",
    "\n",
    "BATCH_SIZE = 64 #Description sets batch size to 64\n",
    "GAMMA = 0.99 #discout factor Description sets to 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 500 #Description: update target_net every 500 games\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_states = 5478\n",
    "n_actions = 9\n",
    "\n",
    "policy_net = DQN(n_states, n_actions) #.to(device)\n",
    "target_net = DQN(n_states, n_actions) #.to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict()) #copy state from policy to target\n",
    "target_net.eval() #put into eval state(BN, Dropout ect.. are now turned off)\n",
    "\n",
    "#Description says to use Adam optimizer with lr = 5*10^-4\n",
    "#optimizer = optim.RMSprop(policy_net.parameters())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=5e-4) \n",
    "#Description gives buffer of 10'000\n",
    "memory = DataSampler(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "#Select action according to epsilon greedy policy.\n",
    "def select_action(state, flag = True): #What does this flag mean ? i set it to false\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_START - EPS_START/num_episodes*(steps_done + 1)\n",
    "    steps_done += 1\n",
    "    if flag:\n",
    "        with torch.no_grad():\n",
    "            possible_moves = np.where(np.ravel(state) == 0)[0]\n",
    "            possible = [int(elem) for elem in np.argsort(np.ravel(policy_net(state).max(1)[1])) \n",
    "                        if elem in possible_moves]\n",
    "            position = possible[-1] if possible is not [] else np.random.choice(possible_moves)\n",
    "            return position\n",
    "    # greedy \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            #return policy_net(state).max(1)[1].view(1, 1)\n",
    "            #TODO implement with unavailable actions like in description\n",
    "            #(i.e can pick wrong actions but has negative reward)\n",
    "            #feed state to policy net and pick possible move with highest q-val\n",
    "            pos = policy_net(state).detach().numpy() #detach maybe not necessary\n",
    "            possible_moves = np.where(np.ravel(state) == 0)[0]\n",
    "            possible = [int(elem) for elem in np.argsort(pos) if elem in possible_moves]\n",
    "            position = possible[-1] if possible is not [] else np.random.choice(possible_moves) \n",
    "            return position\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]],dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    #transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    #batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          np.ravel(batch[\"next_state\"][-1]))), dtype=torch.bool)\n",
    "    data = [s for s in batch[\"next_state\"][-1] if s != [None] * 9 and s is not None and s[0] is not None]\n",
    "    if len(data) > 0:\n",
    "        #print(batch[\"next_state\"][-1])\n",
    "        non_final_next_states = torch.stack(data)\n",
    "    else:\n",
    "        non_final_next_states = torch.tensor([0])\n",
    "    state_batch = torch.tensor(batch[\"state\"][-1])\n",
    "    action_batch = torch.tensor(batch[\"action\"][-1])\n",
    "    reward_batch = torch.tensor(batch[\"reward\"][-1])\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch)[1, action_batch]\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(9) #, device=device)\n",
    "    next_state_values[non_final_mask] = max(target_net(non_final_next_states).max(1)[0].detach())\n",
    "    #target_net(non_final_next_states).max(1)[0].detach() \n",
    "    # Compute the expected Q values\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss() #Huber loss with delta set to 1\n",
    "#     print(state_action_values, expected_state_action_values)\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "#     loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'X']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 57'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=28'>29</a>\u001b[0m                 move \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(position \u001b[39m/\u001b[39m \u001b[39m3\u001b[39m), \u001b[39mint\u001b[39m(position) \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=29'>30</a>\u001b[0m         \u001b[39melse\u001b[39;00m:  \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=30'>31</a>\u001b[0m                 move \u001b[39m=\u001b[39m player\u001b[39m.\u001b[39;49mact(grid)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=31'>32</a>\u001b[0m \u001b[39m#         print(\"My player is:\", Turns[0])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=32'>33</a>\u001b[0m         grid, end, winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(move, print_grid\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:324\u001b[0m, in \u001b[0;36mOptimalPlayer.act\u001b[0;34m(self, grid, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=321'>322</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fork[random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(fork)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=322'>323</a>\u001b[0m \u001b[39m# Block fork\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=323'>324</a>\u001b[0m block_fork \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblockFork(grid)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m block_fork \u001b[39m!=\u001b[39m []:\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=325'>326</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m block_fork[random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(block_fork)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:246\u001b[0m, in \u001b[0;36mOptimalPlayer.blockFork\u001b[0;34m(self, grid)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=239'>240</a>\u001b[0m \u001b[39m''' Block the opponent's fork.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=240'>241</a>\u001b[0m \u001b[39m    If there is only one possible fork from opponent, block it.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=241'>242</a>\u001b[0m \u001b[39m    Otherwise, player should force opponent to block win by making two in a row or column\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=242'>243</a>\u001b[0m \u001b[39m    Amomg all possible force win positions, choose positions in opponent's fork in prior\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=243'>244</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=244'>245</a>\u001b[0m oppon_val \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=245'>246</a>\u001b[0m oppon_fork \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfork(grid, oppon_val)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=246'>247</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(oppon_fork) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=247'>248</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m oppon_fork\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:234\u001b[0m, in \u001b[0;36mOptimalPlayer.fork\u001b[0;34m(self, grid, val)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=231'>232</a>\u001b[0m     grid_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(grid)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=232'>233</a>\u001b[0m     grid_[pos] \u001b[39m=\u001b[39m val\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=233'>234</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckFork(grid_, val):\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=234'>235</a>\u001b[0m         tofork\u001b[39m.\u001b[39mappend(pos)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=236'>237</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tofork\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:288\u001b[0m, in \u001b[0;36mOptimalPlayer.checkFork\u001b[0;34m(self, grid, val)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=285'>286</a>\u001b[0m cols \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39msum(grid, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m target)\u001b[39m.\u001b[39msum()\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=286'>287</a>\u001b[0m diags \u001b[39m=\u001b[39m (grid[[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m]]\u001b[39m.\u001b[39msum() \u001b[39m==\u001b[39m target) \u001b[39m+\u001b[39m (grid[[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m],[\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39msum() \u001b[39m==\u001b[39m target)\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=287'>288</a>\u001b[0m \u001b[39mif\u001b[39;00m (rows \u001b[39m+\u001b[39;49m cols \u001b[39m+\u001b[39;49m diags) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=288'>289</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=289'>290</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Main Training loop\n",
    "\n",
    "other_player_epsilon = 0.\n",
    "num_episodes = 10000\n",
    "Turns = np.array(['X','O'])\n",
    "Turns = np.roll(Turns, 1)\n",
    "print(Turns)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    steps_done = i_episode\n",
    "   \n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    grid, end, winner = env.observe()\n",
    "   \n",
    "    Turns = np.roll(Turns, 1) #first player changes each game\n",
    "    \n",
    "    player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "    \n",
    "    for t in count():\n",
    "        state = np.ravel(grid)\n",
    "        state = torch.from_numpy(state)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        state = state.unsqueeze(0).type(torch.float32)\n",
    "       \n",
    "        # Select and perform an action\n",
    "        if env.current_player == Turns[0]:\n",
    "                position = select_action(state)\n",
    "                move = (int(position / 3), int(position) % 3)\n",
    "        else:  \n",
    "                move = player.act(grid)\n",
    "#         print(\"My player is:\", Turns[0])\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        reward = torch.tensor([[env.reward(Turns[0])]])\n",
    "\n",
    "        # Observe new state\n",
    "        if not end:\n",
    "            next_state = np.ravel(grid)\n",
    "            next_state = torch.from_numpy(next_state) #TODO figure out what exactly is done here\n",
    "            next_state = next_state.unsqueeze(0).type(torch.float32)\n",
    "        else:\n",
    "            next_state = [None] * 9 \n",
    "        \n",
    "        #TODO \n",
    "        #From description: if we are in an illegal state, we give reward = -1.\n",
    "        #instead of forcing player to make legal moves\n",
    "\n",
    "        # Store the transition in memory\n",
    "        action = torch.tensor([[move[0] * 3 + move[1]]]).long() #move\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        if end:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "#     print(f'Completed stage {i_episode+1}')\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_eval(other_player_epsilon=0.5, games=20000):\n",
    "    total_reward = []\n",
    "    env.reset()\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win, none_win, loose = 0, 0, 0 \n",
    "    winner_list = [] \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        \n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                state = np.array([[grid]], dtype=np.int)\n",
    "                state = torch.from_numpy(state)\n",
    "                state = state.unsqueeze(0).type(torch.float32)\n",
    "                position = select_action(state)\n",
    "                move = (int(position / 3), int(position) % 3)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            \n",
    "            if end:\n",
    "                if winner == Turns[0]: \n",
    "                    count_win += 1 \n",
    "                elif str(winner) == \"None\":\n",
    "                    none_win += 1\n",
    "                else:\n",
    "                    loose += 1\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ', Turns[0])\n",
    "#                 print(f'Another player with epsilon {other_player_epsilon} = ', Turns[1])\n",
    "        #             env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "    return count_win / games, none_win / games, loose / games, np.mean(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_15575/2334205948.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  state = np.array([[grid]], dtype=np.int)\n",
      "100%|██████████| 100/100 [00:00<00:00, 390.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins of DQN: 0.49, wins None: 0.02, wins Another player: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = dqn_eval(0.5, 100)\n",
    "print(f\"Wins of DQN: {score[0]}, wins None: {score[1]}, wins Another player: {score[2]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
