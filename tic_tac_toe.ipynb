{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "\n",
    "coords = lambda pos: (int(pos / 3), pos % 3) \n",
    "\n",
    "def eps_greedy(Q, state, grid, epsilon):\n",
    "    possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "    if np.random.random() < epsilon:\n",
    "        position = np.random.choice(possible_moves)\n",
    "    else:\n",
    "        possible = [int(elem) for elem in np.argsort(Q[state]) if elem in possible_moves]\n",
    "        position = possible[-1] if possible is not [] else np.random.choice(possible_moves) \n",
    "    return (int(position / 3), position % 3) \n",
    "\n",
    "#Q learning with greedy\n",
    "#Dont necessariliy need to train with player_epsilon = 0.5\n",
    "def q_learning(env, num_episodes, discount_factor=0.99, alpha=0.05, epsilon=0.3, player_epsilon=0.5):\n",
    "    Q = defaultdict(lambda: np.zeros(9))\n",
    "    episode_lengths = np.zeros(num_episodes)\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    Turns = np.array(['X','O'])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            episode_lengths[i_episode] += 1\n",
    "            if env.current_player == Turns[0]:\n",
    "                #move = eps_greedy(Q, str(grid), grid, epsilon=epsilon - epsilon/num_episodes*i_episode)\n",
    "                \n",
    "                #Remark: for question 2.1 epsilon needs to be constant and not decreasing\n",
    "                move = eps_greedy(Q, str(grid), grid, epsilon = epsilon)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            \n",
    "            new_grid, end, winner = env.step(move, print_grid=False)\n",
    "            episode_rewards[i_episode] += env.reward(env.current_player)\n",
    "#             if end: \n",
    "#                 Q[str(grid)][move[0] * 3 + move[1]] = env.reward(Turns[0])\n",
    "#             else: \n",
    "#             td_target = (discount_factor * np.max(Q[str(new_grid)])) * alpha \n",
    "#             td = (1 - alpha) * Q[str(grid)][move[0] * 3 + move[1]]\n",
    "#             Q[str(grid)][move[0] * 3 + move[1]] = td + td_target \n",
    "            if env.current_player == Turns[0]:\n",
    "                value = Q[str(grid)][move[0] * 3 + move[1]]\n",
    "                Q[str(grid)][move[0] * 3 + move[1]] += alpha * (env.reward(Turns[0]) + discount_factor * np.max(Q[str(new_grid)]) - value)\n",
    "            grid = new_grid \n",
    "    \n",
    "    return Q, episode_lengths, episode_rewards\n",
    "\n",
    "Q, episode_lengths, episode_rewards = q_learning(env, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uodate Q values at every move (optimal player and q-learning player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def q_eval(Q, other_player_epsilon=0.5, games=20000):\n",
    "    env.reset()\n",
    "    total_reward = []\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win = 0 \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                possible_moves = np.where(np.ravel(grid) == 0)[0]\n",
    "                possible = [int(elem) for elem in np.argsort(Q[str(grid)]) if elem in possible_moves]\n",
    "                position = possible[-1] if possible is not [] else np.random.choice(possible_moves)\n",
    "                move = (int(position / 3), position % 3) \n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            if winner == Turns[0]: \n",
    "                count_win += 1 \n",
    "            if end:\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ' +  Turns[0])\n",
    "#                 print('Optimal player = ' +  Turns[1])\n",
    "#                 env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "        \n",
    "    return np.mean(total_reward), count_win / games \n",
    "\n",
    "\n",
    "reward, score = q_eval(Q, other_player_epsilon = 1., games = 2000)\n",
    "print(score, reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#Modifies state to specification given in project description.\n",
    "#Takes a grid and returns a 3x3x2 tensor which contains only values 0 or 1\n",
    "# such that [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent\n",
    "def modify_state_to_description(grid, my_turn_sign): \n",
    "    if('X' == my_turn_sign): #if i am first player to play\n",
    "        my_array =  np.where(grid == -1, 0, grid) #replace -1's with 0(remove other players moves)\n",
    "        other_array = np.where(grid == 1, 0, grid)  #replace 1's with 0\n",
    "        other_array = np.where(other_array == -1, 1, other_array) \n",
    "    else: #-1 = my_moves\n",
    "        my_array = np.where(grid == 1, 0, grid) #remove other players moves\n",
    "        my_array = np.where(my_array == -1, 1, my_array) #change -1 to 1's \n",
    "        other_array = np.where(grid == -1, 0, grid)\n",
    "    #print(\"grid\")#print(grid)\n",
    "    state = np.stack((my_array, other_array))\n",
    "    #print(\"state\")print(state)\n",
    "    return state\n",
    "\n",
    "#Given the state, prepares a tensor for the neural network\n",
    "def get_tensor_for_neural_net(state):\n",
    "    state = torch.from_numpy(state)\n",
    "    state = torch.flatten(state)\n",
    "    return state.unsqueeze(0).type(torch.float32) \n",
    "\n",
    "#Given integer representatin of a move, return tuple representation of move\n",
    "def get_move_from_position(position):\n",
    "    move = (int(position / 3), int(position) % 3)\n",
    "    return move\n",
    "\n",
    "def move_is_legal(move, grid):\n",
    "    possible_moves = np.where(np.ravel(grid.ravel()) == 0)[0]   \n",
    "    return move[0]*3 + move[1] in possible_moves\n",
    "\n",
    "#first player alternates each game\n",
    "def assign_players(turns, i_episode):\n",
    "    if(i_episode > 0):\n",
    "        turns = np.roll(turns, 1) \n",
    "    return turns, turns[0], turns[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Neural net from description\n",
    "# (fully connected, 2 hidden layers w 128 neurons each and relu, 3x3x2 inputs and 9 outputs)\n",
    "# (output layer uses linear activation function)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs=3*3*2, n_outputs=9):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_outputs)\n",
    "\n",
    "    # From description states are 3 ×3 × 2 tensor where\n",
    "    # where [:, :, 0] are moves taken by me, [: ,:, 1] moves taken by opponent(here they are flattened)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(state)\n",
    "            #print(output)\n",
    "            return output.max(1)[1].view(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNActor(): \n",
    "    \n",
    "    def __init__(self, device, batch_size, gamma, memory_size):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        \n",
    "        self.policy = Policy().to(self.device)\n",
    "        self.target = Policy().to(self.device)\n",
    "        self.target.load_state_dict(self.policy.state_dict()) #copy state from policy to target\n",
    "        self.target.eval() #put into eval state(BN, Dropout ect.. are now turned off)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=5e-4) #Description: Adam optimizer with lr = 5e-4\n",
    "\n",
    "        self.latest_loss = 0\n",
    "        \n",
    "\n",
    "    # Taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=self.device,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        #Store lastest loss for plot\n",
    "        self.latest_loss = loss.detach().numpy().item()\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    #Select action according to epsilon greedy policy.\n",
    "    def act(self, state: torch.tensor, eps_min, eps_max, n, num_exploratory_games, fixed_epsilon, fixed_eps_threshold): \n",
    "        sample = random.random()\n",
    "        \n",
    "        #Epsilon update function given in description\n",
    "        if(fixed_epsilon):\n",
    "            eps_threshold = fixed_eps_threshold\n",
    "        else: \n",
    "            eps_threshold = max(eps_min, eps_max*(1 - n/num_exploratory_games))       \n",
    "        # greedy \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return get_move_from_position(self.policy.act(state)), 0\n",
    "        else:\n",
    "            return get_move_from_position(torch.tensor([[random.randrange(9)]],dtype=torch.long)), 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to evaluate DQN against random and against optimal player\n",
    "\n",
    "#Given a DQNActor Compute Mopt and Mrand(Defined in project description)\n",
    "def compute_test_Mrand_Mopt(actor1 : DQNActor):\n",
    "    n_games = 500\n",
    "    #print(\"Mrand: \")\n",
    "    Mrand = eval_against_optimal_player(actor1, other_player_epsilon = 1., n_games=n_games)\n",
    "    #print(\"Mopt: \")\n",
    "    Mopt = eval_against_optimal_player(actor1, other_player_epsilon = 0., n_games=n_games) \n",
    "    \n",
    "    return Mrand, Mopt#Mrand, Mopt\n",
    "    \n",
    "#Evaluate DQNActor over n_games with agains Optimal player with given epsilon and output (wins-losses)/n_games\n",
    "def eval_against_optimal_player(actor1: DQNActor , other_player_epsilon, n_games = 500):\n",
    "    Turns = np.array(['X','O'])\n",
    "    was_invalid = False\n",
    "    env = TictactoeEnv()\n",
    "    losses, wins, invalid_moves, ties = 0, 0, 0 ,0\n",
    "    for i_episode in range(n_games):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns, player1, player2 = assign_players(Turns, i_episode) #first player changes every game\n",
    "        \n",
    "        other_player = OptimalPlayer(other_player_epsilon, player=player2)\n",
    "        \n",
    "        #Loop until game ends\n",
    "        for t in count():\n",
    "            #print(\"grid\")#print(grid)#print(\"state\")#print(state)\n",
    "            state = get_tensor_for_neural_net(modify_state_to_description(np.array(grid, dtype=np.float32), env.current_player))\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            if env.current_player == player1:\n",
    "                #No random moves for evaluation(eps_min, num_exploratory games ect.. not used here)\n",
    "                move, _ = actor1.act(state,  eps_min=0, eps_max=0, n=0, num_exploratory_games=0, fixed_epsilon=True, fixed_eps_threshold=0) \n",
    "            else:  \n",
    "                move = other_player.act(grid)\n",
    "\n",
    "            if move_is_legal(move, grid) == False:\n",
    "                end, was_invalid = True, True\n",
    "            else:\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "            # Observe next state\n",
    "            if not end:\n",
    "                next_state = modify_state_to_description(np.array(grid, dtype=np.float32), env.current_player)  \n",
    "                next_state = get_tensor_for_neural_net(next_state)\n",
    "            #game has ended\n",
    "            else:\n",
    "                #count wins losses\n",
    "                if was_invalid == False:\n",
    "                    if winner == player1: \n",
    "                        wins += 1 \n",
    "                    elif str(winner) == \"None\":\n",
    "                        ties += 1\n",
    "                    else:\n",
    "                        losses += 1\n",
    "                else: \n",
    "                    invalid_moves += 1 #Dont care about invalid moves for this evaluation\n",
    "                was_invalid = False #reset boolean\n",
    "                next_state = None #because finish game\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if end:\n",
    "                break\n",
    "    \n",
    "    #print(\"wins: \" + str(wins) + \", losses: \" + str(losses) + \", ties: \" + str(ties) + \", invalid moves: \" + str(invalid_moves))\n",
    "    return (wins - losses)/(n_games)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "#Training setup (default settings, this changes slightly depending on question)\n",
    "NUM_GAMES = 20000 # Description asks to train on 20k games\n",
    "BATCH_SIZE = 64     # Description sets batch size to 64\n",
    "GAMMA = 0.99        # Description sets discount factor to 0.99\n",
    "EPS_MAX = 0.8       # Description suggests this value\n",
    "EPS_MIN = 0.1       # Description suggests this value\n",
    "NUM_EXPLORATORY_GAMES = 15000 # Play with this value between [1, 40000], has big impact on how fast we learn\n",
    "TARGET_UPDATE = 500 # Description: update target_net every 500 games\n",
    "MEMORY_SIZE = 10000 # Description sets memory size to 10000\n",
    "OTHER_PLAYER_EPSILON = 0.5\n",
    "FIXED_EPS_THRESHOLD = 0.3 # Some questions ask for fixed epsilon\n",
    "FIXED_EPSILON = False     \n",
    "\n",
    "\n",
    "def train(num_games = NUM_GAMES, batch_size = BATCH_SIZE, gamma = GAMMA, eps_max = EPS_MAX, \n",
    "    eps_min = EPS_MIN, num_exploratory_games = NUM_EXPLORATORY_GAMES, target_update = TARGET_UPDATE, \n",
    "    memory_size = MEMORY_SIZE, other_player_epsilon = OTHER_PLAYER_EPSILON, \n",
    "    fixed_eps_threshold = FIXED_EPS_THRESHOLD, fixed_epsilon = FIXED_EPSILON, evaluate_M_values = False\n",
    "    ):\n",
    "\n",
    "    wins, ties, loses, invalid_moves, random_moves, total_moves_player_1 = 0, 0, 0, 0, 0, 0\n",
    "    was_invalid = False\n",
    "    Turns = np.array(['X','O'])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    actor1 = DQNActor(device = device, batch_size = batch_size, gamma = gamma, memory_size = memory_size);\n",
    "\n",
    "    env = TictactoeEnv()\n",
    "\n",
    "    #Keep track of reward and losses for plots\n",
    "    episode_durations = []\n",
    "    rewards = []\n",
    "    losses = [] \n",
    "    Mopts = []\n",
    "    Mrands = []\n",
    "\n",
    "    # Main Training loop\n",
    "    # Train over a number of games\n",
    "    for i_episode in range(num_games):\n",
    "    \n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        grid, end, winner = env.observe()\n",
    "        Turns, player1, player2 = assign_players(Turns, i_episode) #first player changes every game #ik it works cuz if dont alternaty works rly well\n",
    "        \n",
    "        actor2 = OptimalPlayer(epsilon=other_player_epsilon, player=player2)\n",
    "        \n",
    "        #Loop until game ends\n",
    "        for t in count():\n",
    "            #print(\"grid\")#print(grid)#print(\"state\")#print(state)\n",
    "            state = get_tensor_for_neural_net(modify_state_to_description(np.array(grid, dtype=np.float32), player1))\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            if env.current_player == player1:\n",
    "                move, rand = actor1.act(state, eps_min, eps_max, i_episode, num_exploratory_games, fixed_epsilon, fixed_eps_threshold) \n",
    "                random_moves += rand\n",
    "                total_moves_player_1  += 1\n",
    "            else:  \n",
    "                move = actor2.act(grid)\n",
    "\n",
    "            #From description: if move puts us in an illegal state, we end and give reward = -1.\n",
    "            #(instead of forcing player to make legal moves)\n",
    "            if move_is_legal(move, grid) == False:\n",
    "                #print(\"not possible move\")\n",
    "                reward = -1\n",
    "                end, was_invalid = True, True\n",
    "            else:\n",
    "                #play move and get reward (Does it make sense to store also other players moves? )\n",
    "                grid, end, winner = env.step(move, print_grid=False)\n",
    "                reward = env.reward(player1) \n",
    "\n",
    "            # Observe next state\n",
    "            if not end:\n",
    "                next_state = modify_state_to_description(np.array(grid, dtype=np.float32), player1)  #probably should be player 1 here\n",
    "                next_state = get_tensor_for_neural_net(next_state)\n",
    "            #game has ended\n",
    "            else:\n",
    "                #count wins losses ties \n",
    "                if was_invalid == False:\n",
    "                    if winner == player1: \n",
    "                        wins += 1 \n",
    "                    elif str(winner) == \"None\":\n",
    "                        ties += 1\n",
    "                    else:\n",
    "                        loses += 1\n",
    "                else: \n",
    "                    invalid_moves += 1\n",
    "                was_invalid = False #reset boolean\n",
    "                next_state = None #because finish game\n",
    "        \n",
    "            #TODO if actor1 is DQNActor or actor2 is DQNActor\n",
    "            # Store the transition in memory\n",
    "       \n",
    "            action = torch.tensor([[move[0] * 3 + move[1]]]) #move\n",
    "            \n",
    "            actor1.memory.push(state, action, next_state, torch.tensor([reward]))\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network) of given actors\n",
    "            actor1.optimize_model()\n",
    "            \n",
    "            if end:\n",
    "                episode_durations.append(t + 1)\n",
    "                rewards.append(reward)\n",
    "                losses.append(actor1.latest_loss)\n",
    "                #print(losses)\n",
    "                break\n",
    "        # Update the target network(s), copying all weights and biases in DQN\n",
    "        if i_episode % target_update == 0:\n",
    "            actor1.target.load_state_dict(actor1.policy.state_dict())\n",
    "        \n",
    "        # Evaluate training\n",
    "        if (i_episode+1) % 5000 == 0 and i_episode != 1:\n",
    "            print(\"For batch of 5000 games: \" + \" Wins: \" + str(wins) + \", Losses: \" + str(loses) +\n",
    "                \", Ties: \" + str(ties) + \" Invalid moves: \" + str(invalid_moves)\n",
    "                + \", Percentage of random moves: \" + str(random_moves/total_moves_player_1))\n",
    "            wins, ties, loses, invalid_moves, random_moves, total_moves_player_1 = 0, 0, 0, 0, 0, 0 \n",
    "        \n",
    "        \n",
    "        #If want to calculate M values every 250 games\n",
    "        if(evaluate_M_values == True and (i_episode+1) % 250 == 0 and i_episode != 1):\n",
    "            Mrand, Mopt = compute_test_Mrand_Mopt(actor1)\n",
    "            Mrands.append(Mrand)\n",
    "            Mopts.append(Mopt)\n",
    "    \n",
    "    print('Complete')\n",
    "    return rewards, losses, Mrands, Mopts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 56'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=0'>1</a>\u001b[0m \u001b[39m#DEMO CELL CAN DELETE AFTER\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=1'>2</a>\u001b[0m \u001b[39m#Plots\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=2'>3</a>\u001b[0m \u001b[39m#Demo plot trains with default parameters\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=3'>4</a>\u001b[0m rewards, losses, _ , _ \u001b[39m=\u001b[39m train(other_player_epsilon\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, num_exploratory_games\u001b[39m=\u001b[39;49m \u001b[39m10000\u001b[39;49m, num_games\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m) \u001b[39m#fixed_eps_threshold=0.3, fixed_epsilon=True\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=4'>5</a>\u001b[0m \u001b[39m#Plot average reward and losses for every 250 games during training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000055?line=5'>6</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m250\u001b[39m\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 55'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_games, batch_size, gamma, eps_max, eps_min, num_exploratory_games, target_update, memory_size, other_player_epsilon, fixed_eps_threshold, fixed_epsilon, evaluate_M_values)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=101'>102</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=103'>104</a>\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network) of given actors\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=104'>105</a>\u001b[0m actor1\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=106'>107</a>\u001b[0m \u001b[39mif\u001b[39;00m end:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=107'>108</a>\u001b[0m     episode_durations\u001b[39m.\u001b[39mappend(t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 53'\u001b[0m in \u001b[0;36mDQNActor.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=65'>66</a>\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=66'>67</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=67'>68</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000052?line=69'>70</a>\u001b[0m     param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclamp_(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/gianni/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DEMO CELL CAN DELETE AFTER\n",
    "#Plots\n",
    "#Demo plot trains with default parameters\n",
    "rewards, losses, _ , _ = train(other_player_epsilon=1.0, num_exploratory_games= 10000, num_games=20000) #fixed_eps_threshold=0.3, fixed_epsilon=True\n",
    "#Plot average reward and losses for every 250 games during training\n",
    "n = 250\n",
    "avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "plt.plot(avg_rewards)\n",
    "#plt.xlabel('250 games') not sure what to name this, 1 unit = 250 games, maybe can scale *250 ? and call it games\n",
    "plt.ylabel('reward')\n",
    "plt.savefig('reward.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "plt.plot(avg_losses)\n",
    "#plt.xlabel('250 games')\n",
    "plt.ylabel('training loss')\n",
    "plt.savefig('loss.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 11\n",
    "#Plot average reward and average training loss for every 250 games during training. Does\n",
    "#the loss decrease? Does the agent learn to play Tic Tac Toe?\n",
    "#Expected answer: A figure with two subplots (caption length < 50 words). Specify your choice of eps.\n",
    "\n",
    "#TODO Question: should we plot them as subplot or each one their own plot then put them together in overleaf?\n",
    "thresholds = [0.05, 0.1, 0.15, 0.2, 0.3, 0.4]\n",
    "#Maybe set title = fixed_eps_threshold\n",
    "for fixed_eps_threshold in thresholds:\n",
    "    figure, axes = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "    rewards, losses, _ , _ = train(fixed_epsilon=True, fixed_eps_threshold=fixed_eps_threshold) \n",
    "    #Plot average reward and losses for every 250 games during training\n",
    "    n = 250 # Average over sets of 250 games\n",
    "    avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "    x = [i*250 for i in range(len(avg_rewards))]\n",
    "    axes[0].plot(x, avg_rewards)\n",
    "    axes[0].set_xlabel('number training games') \n",
    "    axes[0].set_ylabel('reward')\n",
    "    #axes[0].savefig('question11_reward_' + str(fixed_eps_threshold) +'.png', bbox_inches='tight')\n",
    "    #plt.show()\n",
    "    avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "    x = [i*250 for i in range(len(avg_losses))]\n",
    "    axes[1].plot(x, avg_losses)\n",
    "    axes[1].set_xlabel('number of training games')\n",
    "    axes[1].set_ylabel('training loss')\n",
    "    figure.savefig('question11_loss_' + str(fixed_eps_threshold) + '.png', bbox_inches='tight')\n",
    "    figure.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 58'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=0'>1</a>\u001b[0m \u001b[39m#TODO This question doesnt work\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=1'>2</a>\u001b[0m \u001b[39m#TODO Question 12, (apperently not as simple as setting memory_size = 1, =/ )\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=2'>3</a>\u001b[0m \u001b[39m#Repeat the training but without the replay buffer and with a batch size of 1: At every\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=5'>6</a>\u001b[0m \u001b[39m#training (caption length < 50 words).\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=6'>7</a>\u001b[0m \u001b[39m#TODO use best fixed_eps computed in Question 11\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=7'>8</a>\u001b[0m rewards, losses, _ , _ \u001b[39m=\u001b[39m train(memory_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, fixed_epsilon\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fixed_eps_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=8'>9</a>\u001b[0m \u001b[39m#Plot average reward and losses for every 250 games during training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000057?line=9'>10</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m250\u001b[39m \u001b[39m# Average over sets of 250 games\u001b[39;00m\n",
      "\u001b[1;32m/home/gianni/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb Cell 55'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_games, batch_size, gamma, eps_max, eps_min, num_exploratory_games, target_update, memory_size, other_player_epsilon, fixed_eps_threshold, fixed_epsilon, evaluate_M_values)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=59'>60</a>\u001b[0m     total_moves_player_1  \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=60'>61</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=61'>62</a>\u001b[0m     move \u001b[39m=\u001b[39m actor2\u001b[39m.\u001b[39;49mact(grid)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=63'>64</a>\u001b[0m \u001b[39m#From description: if move puts us in an illegal state, we end and give reward = -1.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=64'>65</a>\u001b[0m \u001b[39m#(instead of forcing player to make legal moves)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=65'>66</a>\u001b[0m \u001b[39mif\u001b[39;00m move_is_legal(move, grid) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_tac_toe.ipynb#ch0000054?line=66'>67</a>\u001b[0m     \u001b[39m#print(\"not possible move\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:312\u001b[0m, in \u001b[0;36mOptimalPlayer.act\u001b[0;34m(self, grid, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=309'>310</a>\u001b[0m \u001b[39m# whether move in random or not\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=310'>311</a>\u001b[0m \u001b[39mif\u001b[39;00m random\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=311'>312</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandomMove(grid)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=313'>314</a>\u001b[0m \u001b[39m# optimial policies\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=314'>315</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=315'>316</a>\u001b[0m \u001b[39m# Win\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=316'>317</a>\u001b[0m win \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwin(grid)\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:300\u001b[0m, in \u001b[0;36mOptimalPlayer.randomMove\u001b[0;34m(self, grid)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=297'>298</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandomMove\u001b[39m(\u001b[39mself\u001b[39m, grid):\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=298'>299</a>\u001b[0m     \u001b[39m\"\"\" Chose a random move from the available options. \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=299'>300</a>\u001b[0m     avail \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mempty(grid)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=301'>302</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m avail[random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(avail)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[0;32m~/Desktop/NEW ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py:169\u001b[0m, in \u001b[0;36mOptimalPlayer.empty\u001b[0;34m(self, grid)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=166'>167</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m9\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=167'>168</a>\u001b[0m     pos \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(i\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m), i \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=168'>169</a>\u001b[0m     \u001b[39mif\u001b[39;00m grid[pos] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=169'>170</a>\u001b[0m         avail\u001b[39m.\u001b[39mappend(pos)\n\u001b[1;32m    <a href='file:///home/gianni/Desktop/NEW%20ANN/CS-456-Artificial-Neural-Network-Miniprojects/tic_env.py?line=170'>171</a>\u001b[0m \u001b[39mreturn\u001b[39;00m avail\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO This question doesnt work\n",
    "#TODO Question 12, \n",
    "#Repeat the training but without the replay buffer and with a batch size of 1: At every\n",
    "#step, update the network by using only the latest transition. What do you observe?\n",
    "#Expected answer: A figure with two subplots showing average reward and average training loss during\n",
    "#training (caption length < 50 words).\n",
    "#TODO use best fixed_eps computed in Question 11\n",
    "#TODO setting batch_size = 1 breaks the algorithm =/\n",
    "rewards, losses, _ , _ = train(memory_size = 1, batch_size = 2, fixed_epsilon=True, fixed_eps_threshold=0.3) \n",
    "#Plot average reward and losses for every 250 games during training\n",
    "n = 250 # Average over sets of 250 games\n",
    "avg_rewards = np.average(np.array(rewards).reshape(-1, n), axis=1)\n",
    "x = [i*250 for i in range(len(rewards))]\n",
    "plt.plot(x, avg_rewards)\n",
    "plt.xlabel('number training games') \n",
    "plt.ylabel('reward')\n",
    "plt.savefig('question11_reward_' + str(fixed_eps_threshold) +'.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "avg_losses = np.average(np.array(losses).reshape(-1, n), axis=1) \n",
    "x = [i*250 for i in range(len(avg_losses))]\n",
    "plt.plot(avg_losses)\n",
    "plt.xlabel('number of training games')\n",
    "plt.ylabel('training loss')\n",
    "plt.savefig('question11_loss_' + str(fixed_eps_threshold) + '.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins: 137, losses: 253, ties: 11, invalid moves: 99\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 253, losses: 207, ties: 17, invalid moves: 23\n",
      "wins: 0, losses: 439, ties: 0, invalid moves: 61\n",
      "wins: 189, losses: 246, ties: 28, invalid moves: 37\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 130, losses: 311, ties: 29, invalid moves: 30\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 184, losses: 263, ties: 13, invalid moves: 40\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 151, losses: 296, ties: 17, invalid moves: 36\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 163, losses: 272, ties: 30, invalid moves: 35\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 181, losses: 254, ties: 17, invalid moves: 48\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 206, losses: 235, ties: 20, invalid moves: 39\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 186, losses: 230, ties: 41, invalid moves: 43\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 195, losses: 241, ties: 21, invalid moves: 43\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 233, losses: 221, ties: 18, invalid moves: 28\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 223, losses: 206, ties: 25, invalid moves: 46\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 225, losses: 220, ties: 14, invalid moves: 41\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 259, losses: 201, ties: 7, invalid moves: 33\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 281, losses: 173, ties: 9, invalid moves: 37\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 207, losses: 205, ties: 16, invalid moves: 72\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 229, losses: 228, ties: 12, invalid moves: 31\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 159, losses: 254, ties: 32, invalid moves: 55\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "For batch of 5000 games:  Wins: 328, Losses: 1854, Ties: 41 Invalid moves: 2777, Percentage of random moves: 0.6699249673022647\n",
      "wins: 230, losses: 200, ties: 29, invalid moves: 41\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 219, losses: 208, ties: 28, invalid moves: 45\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 220, losses: 217, ties: 20, invalid moves: 43\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 202, losses: 237, ties: 10, invalid moves: 51\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 212, losses: 224, ties: 24, invalid moves: 40\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 215, losses: 222, ties: 6, invalid moves: 57\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 235, losses: 181, ties: 15, invalid moves: 69\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 214, losses: 230, ties: 22, invalid moves: 34\n",
      "wins: 0, losses: 250, ties: 0, invalid moves: 250\n",
      "wins: 222, losses: 208, ties: 25, invalid moves: 45\n",
      "wins: 0, losses: 250, ties: 250, invalid moves: 0\n",
      "wins: 246, losses: 188, ties: 12, invalid moves: 54\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 289, losses: 159, ties: 21, invalid moves: 31\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 216, losses: 199, ties: 14, invalid moves: 71\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 206, losses: 200, ties: 32, invalid moves: 62\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 232, losses: 210, ties: 16, invalid moves: 42\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 235, losses: 181, ties: 18, invalid moves: 66\n",
      "wins: 0, losses: 407, ties: 93, invalid moves: 0\n",
      "wins: 241, losses: 180, ties: 18, invalid moves: 61\n",
      "wins: 0, losses: 445, ties: 0, invalid moves: 55\n",
      "wins: 287, losses: 151, ties: 16, invalid moves: 46\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 235, losses: 163, ties: 36, invalid moves: 66\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 279, losses: 110, ties: 37, invalid moves: 74\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 242, losses: 148, ties: 48, invalid moves: 62\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "For batch of 5000 games:  Wins: 679, Losses: 2222, Ties: 120 Invalid moves: 1979, Percentage of random moves: 0.3950166329658861\n",
      "wins: 281, losses: 138, ties: 31, invalid moves: 50\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 180, losses: 218, ties: 32, invalid moves: 70\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 187, losses: 214, ties: 30, invalid moves: 69\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 298, losses: 144, ties: 20, invalid moves: 38\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 280, losses: 140, ties: 37, invalid moves: 43\n",
      "wins: 0, losses: 434, ties: 66, invalid moves: 0\n",
      "wins: 254, losses: 177, ties: 17, invalid moves: 52\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 239, losses: 179, ties: 21, invalid moves: 61\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 241, losses: 171, ties: 17, invalid moves: 71\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 249, losses: 171, ties: 14, invalid moves: 66\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 264, losses: 160, ties: 19, invalid moves: 57\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 285, losses: 163, ties: 9, invalid moves: 43\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 258, losses: 157, ties: 26, invalid moves: 59\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 267, losses: 144, ties: 38, invalid moves: 51\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 219, losses: 192, ties: 24, invalid moves: 65\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 238, losses: 202, ties: 26, invalid moves: 34\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 257, losses: 174, ties: 18, invalid moves: 51\n",
      "wins: 0, losses: 453, ties: 47, invalid moves: 0\n",
      "wins: 307, losses: 128, ties: 30, invalid moves: 35\n",
      "wins: 0, losses: 448, ties: 52, invalid moves: 0\n",
      "wins: 204, losses: 211, ties: 31, invalid moves: 54\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 234, losses: 189, ties: 28, invalid moves: 49\n",
      "wins: 0, losses: 453, ties: 47, invalid moves: 0\n",
      "wins: 256, losses: 178, ties: 18, invalid moves: 48\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "For batch of 5000 games:  Wins: 1031, Losses: 2849, Ties: 216 Invalid moves: 904, Percentage of random moves: 0.14480430161310492\n",
      "wins: 236, losses: 180, ties: 18, invalid moves: 66\n",
      "wins: 0, losses: 449, ties: 51, invalid moves: 0\n",
      "wins: 259, losses: 177, ties: 32, invalid moves: 32\n",
      "wins: 0, losses: 447, ties: 53, invalid moves: 0\n",
      "wins: 253, losses: 203, ties: 17, invalid moves: 27\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 237, losses: 189, ties: 28, invalid moves: 46\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 273, losses: 169, ties: 24, invalid moves: 34\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 259, losses: 175, ties: 11, invalid moves: 55\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 238, losses: 202, ties: 17, invalid moves: 43\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 270, losses: 169, ties: 31, invalid moves: 30\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 281, losses: 145, ties: 34, invalid moves: 40\n",
      "wins: 0, losses: 457, ties: 43, invalid moves: 0\n",
      "wins: 267, losses: 175, ties: 15, invalid moves: 43\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 281, losses: 139, ties: 22, invalid moves: 58\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 224, losses: 168, ties: 32, invalid moves: 76\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 232, losses: 173, ties: 42, invalid moves: 53\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 228, losses: 172, ties: 35, invalid moves: 65\n",
      "wins: 0, losses: 393, ties: 0, invalid moves: 107\n",
      "wins: 250, losses: 179, ties: 36, invalid moves: 35\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 266, losses: 153, ties: 43, invalid moves: 38\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 251, losses: 155, ties: 52, invalid moves: 42\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 290, losses: 158, ties: 31, invalid moves: 21\n",
      "wins: 0, losses: 456, ties: 44, invalid moves: 0\n",
      "wins: 273, losses: 143, ties: 38, invalid moves: 46\n",
      "wins: 0, losses: 500, ties: 0, invalid moves: 0\n",
      "wins: 267, losses: 146, ties: 39, invalid moves: 48\n",
      "wins: 0, losses: 401, ties: 99, invalid moves: 0\n",
      "For batch of 5000 games:  Wins: 1130, Losses: 2866, Ties: 263 Invalid moves: 741, Percentage of random moves: 0.10166177908113393\n",
      "wins: 254, losses: 174, ties: 32, invalid moves: 40\n",
      "wins: 0, losses: 451, ties: 49, invalid moves: 0\n",
      "Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4921/1905753263.py:26: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  figure.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Instead of fixing eps, use eps(n) in Equation 1. For different values of n∗, run your DQN against Opt(0.5)\n",
    "#for 20’000 games – switch the 1st player after every game. Choose several values of n∗from a reasonably\n",
    "#wide interval between 1 to 40’000 – particularly, include n∗= 1.\n",
    "\n",
    "#TODO create loop to test different n* values (num_exploratory_games)\n",
    "#Question 13\n",
    "# After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents.\n",
    "#Plot Mopt and Mrand over time. Does decreasing eps help training compared to having a fixed eps? What is\n",
    "#the effect of n∗?\n",
    "#TODO plots both these lines on the same plot!!!\n",
    "figure, axes = plt.subplots(nrows=2, ncols=1)\n",
    "#TODO test different num_eploratory_games values\n",
    "_, _, Mrands , Mopts  = train(num_games = 20000, num_exploratory_games= 15000, evaluate_M_values=True) \n",
    "#Plot average reward and losses for every 250 games during training\n",
    "n = 250 # Average over sets of 250 games\n",
    "x = [i*250 for i in range(len(Mrands))]\n",
    "axes[0].plot(x, Mrands)\n",
    "axes[0].set_xlabel('number training games') \n",
    "axes[0].set_ylabel('Mrand')\n",
    "axes[1].plot(x, Mopts)\n",
    "axes[1].set_xlabel('number of training games')\n",
    "axes[1].set_ylabel('Mopt ')\n",
    "figure.savefig('question13_Mrand_Mopt_2.png', bbox_inches='tight')\n",
    "figure.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the best value of n∗ that you found. Run DQN against Opt(eps_opt) for different values of eps_opt for\n",
    "#20’000 games – switch the 1st player after every game. Choose several values of eps_opt from a reasonably\n",
    "#wide interval between 0 to 1 – particularly, include eps_opt = 0.\n",
    "\n",
    "#Question 14. After every 250 games during training, compute the ‘test’ Mopt and Mrand for your agents\n",
    "#for each value of eps_opt. Plot Mopt and Mrand over time. What do you observe? How can you explain it?\n",
    "#Expected answer: A figure showing Mopt and Mrand over time for different values of eps_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO How to evaluate invalid moves ? think maybe only need this for question 15.\n",
    "def dqn_eval(other_player_epsilon=0.5, games=20000):\n",
    "    total_reward = []\n",
    "    env.reset()\n",
    "    Turns = np.array(['X','O'])\n",
    "    count_win, none_win, loose = 0, 0, 0 \n",
    "    winner_list = [] \n",
    "    for i in tqdm(range(games)):\n",
    "        grid, end, winner = env.observe()\n",
    "        \n",
    "        player = OptimalPlayer(epsilon=other_player_epsilon, player=Turns[1])\n",
    "        while not end:\n",
    "            if env.current_player == Turns[0]:\n",
    "                state = modify_state_to_description(np.array([[grid]], dtype=np.int32), Turns[0])\n",
    "                state = torch.from_numpy(state)\n",
    "                # Resize, and add a batch dimension (BCHW)\n",
    "                state = state.unsqueeze(0).type(torch.float32) #mmh suspicious\n",
    "                position = select_model_action(policy, state)\n",
    "                move = (int(position / 3), int(position) % 3)\n",
    "            else:  \n",
    "                move = player.act(grid)\n",
    "            #at this point should not be making invalid moves... because player has trained\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "            total_reward.append(env.reward(env.current_player))\n",
    "            \n",
    "            if end:\n",
    "                if winner == Turns[0]: \n",
    "                    count_win += 1 \n",
    "                elif str(winner) == \"None\":\n",
    "                    none_win += 1\n",
    "                else:\n",
    "                    loose += 1\n",
    "#                 print('-------------------------------------------')\n",
    "#                 print('Game end, winner is player ' + str(winner))\n",
    "#                 print('Q-Learning player = ', Turns[0])\n",
    "#                 print(f'Another player with epsilon {other_player_epsilon} = ', Turns[1])\n",
    "        #             env.render()\n",
    "                env.reset()\n",
    "                break\n",
    "    return count_win / games, none_win / games, loose / games, np.mean(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = dqn_eval(0.5, 1000)\n",
    "print(f\"Wins of DQN: {score[0]}, wins None: {score[1]}, wins Another player: {score[2]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
